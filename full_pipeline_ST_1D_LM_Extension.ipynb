{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH84VsOfzArU"
      },
      "source": [
        "# Setup\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFVsD2Aa2gvW"
      },
      "source": [
        "## Google Drive Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeQiYtKS2kxd",
        "outputId": "c6b2e522-7e20-4423-cf5a-6cc1418c350a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "##  @brief  :   Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpI_0WOizKYD"
      },
      "source": [
        "## Libraries and Installations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQmijF9ezVlf"
      },
      "source": [
        "### **Set up Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIZ7djjH1ato",
        "outputId": "d78791af-012d-4c5c-c16d-e35385ddb032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.3/510.3 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Collecting git+https://github.com/facebookresearch/esm.git\n",
            "  Cloning https://github.com/facebookresearch/esm.git to /tmp/pip-req-build-sonov955\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/esm.git /tmp/pip-req-build-sonov955\n",
            "  Resolved https://github.com/facebookresearch/esm.git to commit 2b369911bb5b4b0dda914521b9475cad1656b2ac\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fair-esm\n",
            "  Building wheel for fair-esm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fair-esm: filename=fair_esm-2.0.1-py3-none-any.whl size=105381 sha256=17999e353047a58ea23f4c55d0f91ff7af7997b383b27badb77fe6075da34bf2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1hocla8/wheels/f3/b2/ec/4db0b108f6367c7563f99b2445e1137d486003fb2f9bfd2f53\n",
            "Successfully built fair-esm\n",
            "Installing collected packages: fair-esm\n",
            "  Attempting uninstall: fair-esm\n",
            "    Found existing installation: fair-esm 2.0.0\n",
            "    Uninstalling fair-esm-2.0.0:\n",
            "      Successfully uninstalled fair-esm-2.0.0\n",
            "Successfully installed fair-esm-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torchsummaryX einops \"fair-esm[esmfold]\" transformers sentencepiece lxml pyfaidx\n",
        "# OpenFold and its remaining dependency\n",
        "!pip install fair-esm  # latest release, OR:\n",
        "!pip install git+https://github.com/facebookresearch/esm.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y556rTUFfKV"
      },
      "outputs": [],
      "source": [
        "!pip install -q protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN4CE5fgzinU"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTjF7Fb_0RhG"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Libraries for File I/O & Parsing\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import pickle\n",
        "import json\n",
        "import csv\n",
        "import argparse\n",
        "from io import StringIO\n",
        "import shutil\n",
        "from urllib.parse import urlparse\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6IXXRas0GKU"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Libraries for Data Analysis / Stats / Pre-processing\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import re\n",
        "import itertools\n",
        "from datetime import datetime, timedelta\n",
        "import pickle\n",
        "from numbers import Number\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as blue_tqdm\n",
        "\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "627rLF7A0BIm"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Torch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import nn, Tensor, einsum\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, unpad_sequence\n",
        "\n",
        "import einops\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from einops import rearrange, repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxelsRl6oBtc"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Keras & TF Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sv4fQcxnrnt"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Imports for External / Pre-Trained PLMs\n",
        "import esm\n",
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "import sentencepiece\n",
        "from lxml import etree\n",
        "from pyfaidx import Faidx\n",
        "\n",
        "\n",
        "##  @brief  :   Import Local Module for ProteinBERT\n",
        "##  @note   :   Replace all instances of log function in module with print\n",
        "proteinBERT_DIR = '/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/ProteinBERT/proteinbert_keras/proteinbert'\n",
        "sys.path.insert(0,proteinBERT_DIR)\n",
        "\n",
        "from tokenization import ADDED_TOKENS_PER_SEQ, index_to_token, token_to_index\n",
        "from model_generation import ModelGenerator, PretrainingModelGenerator, FinetuningModelGenerator, InputEncoder, load_pretrained_model_from_dump, tokenize_seqs\n",
        "from existing_model_loading import load_pretrained_model\n",
        "from finetuning import OutputType, OutputSpec, finetune, evaluate_by_len\n",
        "from conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlNDZkxWnwA6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTCiI5mH2LuB"
      },
      "source": [
        "## Compute Specifications\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-smPkP52ZS5",
        "outputId": "643a7b74-737c-4bc5-f88b-116a3e43b210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 10 16:04:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "##  @brief  :   Check GPU/CPU Specs\n",
        "!nvidia-smi\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYZOR0GC18d-"
      },
      "source": [
        "## W&B\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ukgj2u22Dt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0e4f55-d19f-4163-fe23-2d77d0bc1f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiDi4mD21-0e",
        "outputId": "9681579a-969f-4816-dba1-da0f80765ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "##  @brief  : W&B Login with API Key\n",
        "wandb.login(key=\"c035e37d4ff69175c6c747cc2c851a8f8d11baab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHg8H1_x2_gI"
      },
      "source": [
        "# File Paths\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8f4wRuQ3yeu"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Root Path to Git Folder\n",
        "ROOT = '/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/SPOT-1D-LM/'\n",
        "\n",
        "MODEL_DATA_ROOT = os.path.join(ROOT, \"spot_1d_lm\")\n",
        "\n",
        "##  @brief  :   Path containing Model Inputs\n",
        "INPUT_DIR = os.path.join(ROOT, \"inputs\")\n",
        "\n",
        "##  @brief  :   Path to Generated 1-hot encoded Sequences\n",
        "ONEHOT_INPUT_DIR = os.path.join(INPUT_DIR, \"one_hot\")\n",
        "\n",
        "##  @brief  :   Path containing Labels\n",
        "LABEL_DIR = os.path.join(MODEL_DATA_ROOT, \"labels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE16F15nxqBE"
      },
      "source": [
        "## Dataset File Paths\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSo2xYrOEcZ8"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Path to Dataset Folder -> Contains Lists of Paths to Fasta Files\n",
        "DATASET_DIR = os.path.join(MODEL_DATA_ROOT, \"lists\")\n",
        "\n",
        "##  @brief  :   Directory containing all available fasta files\n",
        "FASTA_DIR = os.path.join(MODEL_DATA_ROOT, \"fasta\")\n",
        "\n",
        "##  @brief  :   Directory to store csvs with processed inputs\n",
        "DF_DIR = os.path.join(INPUT_DIR, \"dataframes\")\n",
        "\n",
        "##  @brief  :   Directory to store compressed embeddings\n",
        "FEAT_DIR = os.path.join(INPUT_DIR, \"compressed_inputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h_-_ekmxuGy"
      },
      "source": [
        "### **Train Sets & Dev Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b81KTDNFPcU"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Path to Train Data\n",
        "TRAIN_FILE = os.path.join(DATASET_DIR, \"train.txt\")\n",
        "TRAIN_DF = os.path.join(DF_DIR, \"train.csv\")\n",
        "TRAIN_INPUTS = os.path.join(FEAT_DIR, \"train\")\n",
        "\n",
        "##  @brief  :   Path to Validation Data\n",
        "VAL_FILE = os.path.join(DATASET_DIR, \"validation.txt\")\n",
        "VAL_DF = os.path.join(DF_DIR, \"validation.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naCVKU5Txxok"
      },
      "source": [
        "### **Test Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYWqeR35FOzt"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Dictionary of Test Files\n",
        "TEST_FILES = {\n",
        "    \"SPOT-2016-HQ\"  : os.path.join(DATASET_DIR, \"SPOT-2016-HQ.txt\"),\n",
        "    \"SPOT-2016\"  : os.path.join(DATASET_DIR, \"SPOT-2016.txt\"),\n",
        "    \"SPOT-2018-HQ\"  : os.path.join(DATASET_DIR, \"SPOT-2018-HQ.txt\"),\n",
        "    \"SPOT-2018\"  : os.path.join(DATASET_DIR, \"SPOT-2018.txt\"),\n",
        "    \"CASP-12\"  : os.path.join(DATASET_DIR, \"casp12.txt\"),\n",
        "    \"CASP-13\"  : os.path.join(DATASET_DIR, \"casp13_fm_17.txt\"),\n",
        "    \"TEST-2018\"  : os.path.join(DATASET_DIR, \"test2018.txt\"),\n",
        "}\n",
        "\n",
        "TEST_DFS = {\n",
        "    \"SPOT-2016-HQ\"  : os.path.join(DF_DIR, \"SPOT-2016-HQ.csv\"),\n",
        "    \"SPOT-2016\"  : os.path.join(DF_DIR, \"SPOT-2016.csv\"),\n",
        "    \"SPOT-2018-HQ\"  : os.path.join(DF_DIR, \"SPOT-2018-HQ.csv\"),\n",
        "    \"SPOT-2018\"  : os.path.join(DF_DIR, \"SPOT-2018.csv\"),\n",
        "    \"CASP-12\"  : os.path.join(DF_DIR, \"casp12.csv\"),\n",
        "    \"CASP-13\"  : os.path.join(DF_DIR, \"casp13_fm_17.csv\"),\n",
        "    \"TEST-2018\"  : os.path.join(DF_DIR, \"test2018.csv\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRfuc2fnxEQ5"
      },
      "source": [
        "## Model File Paths\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ06xW5q0Ber"
      },
      "source": [
        "### **Model Checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuPGH_G60QeC"
      },
      "outputs": [],
      "source": [
        "CHECKPOINTS_DIR = os.path.join(MODEL_DATA_ROOT, \"checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFboghkm0J_e"
      },
      "source": [
        "Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A78MUlfi0FxV"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Path to ESM-1b Checkpoints\n",
        "ESM_CHECKPOINTS_DIR = os.path.join(CHECKPOINTS_DIR, \"esm-1b\")\n",
        "\n",
        "##  @brief  :   Path to ProteinBERT\n",
        "PB_CHECKPOINTS_DIR = os.path.join(CHECKPOINTS_DIR, \"proteinBERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrArcL2FFHeZ"
      },
      "source": [
        "### **Model Embeddings**\n",
        "*Embeddings from Pre-Trained Model to Use as Inputs to Spot-1D-Single*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1SAHHq3FyaR"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Path to Generated ESM-1b Embeddings\n",
        "ESM1B_INPUT_DIR = os.path.join(INPUT_DIR, \"esm\")\n",
        "\n",
        "##  @brief  :   Path to Generated ProtTrans Embeddings\n",
        "PT_INPUT_DIR = os.path.join(INPUT_DIR, \"protTrans\")\n",
        "\n",
        "##  @brief  :   Path to Generated ProteinBERT Embeddings\n",
        "PB_INPUT_DIR = os.path.join(INPUT_DIR, \"proteinBERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfEaGpqMFhDt"
      },
      "source": [
        "## Label File Paths\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_B7aoE8FwyB"
      },
      "outputs": [],
      "source": [
        "##  @brief  :   Directory containing all available dssp ground truth files\n",
        "DSSP_DIR = os.path.join(MODEL_DATA_ROOT, \"dssp\")\n",
        "\n",
        "##  @brief  :   Directory containing all available theta ground truth files\n",
        "THETA_DIR = os.path.join(MODEL_DATA_ROOT, \"theta\")\n",
        "\n",
        "##  @brief  :   Directory containing all available hse ground truth files\n",
        "HSE_DIR = os.path.join(MODEL_DATA_ROOT, \"hse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjClyCbWmCVT"
      },
      "source": [
        "#Configurations\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnOuVRa2mFwY"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    file_list_train = \"spot_1d_lm/lists/train.txt\",\n",
        "    file_list_val   = \"spot_1d_lm/lists/val.txt\",\n",
        "    file_list_test  = \"spot_1d_lm/lists/casp12.txt\",\n",
        "    embedding1      = \"esm-1b\",\n",
        "    embedding2      = \"proteinBERT\",\n",
        "    batch_size      = 10,\n",
        "    epochs           = 100,\n",
        "    loss            = torch.nn.CrossEntropyLoss(),\n",
        "    device          = \"cuda:3\",\n",
        "    optimizer       = 'adam',\n",
        "    weight_decay   = 1e-4,\n",
        "    momentum        = 0.9,\n",
        "    lr   = 2e-4,\n",
        "    run             = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfAjneXmy7Ku"
      },
      "outputs": [],
      "source": [
        "CURRENT_CONFIG = config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhXkXsM5V9b"
      },
      "source": [
        "# Pre-Trained Models\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fahYsxJ6wskT"
      },
      "source": [
        "## ESM-1b\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewgrrw634EtI"
      },
      "outputs": [],
      "source": [
        "ESM_EMBEDDING_DIM = 1280"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neOxntNrfKlP"
      },
      "outputs": [],
      "source": [
        "class PreTrainedESM1b():\n",
        "    \"\"\"!  Pre Trained ESM-1b Model Class\n",
        "\n",
        "          Used to Access Pre-Trained model and generate embeddings for datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, input_path):\n",
        "        \"\"\"!  Pre Trained ESM-1b Model Class initializer\n",
        "\n",
        "              @param[in]  :   model_path = path to saved model checkpoints\n",
        "              @param[in]  :   input_path = path to saved embeddings\n",
        "        \"\"\"\n",
        "        self.input_path = input_path\n",
        "\n",
        "        self.embedding_dim = ESM_EMBEDDING_DIM\n",
        "\n",
        "        ## Load Instance of Pre-Trained ESM-1b Model\n",
        "        self.model, self.alphabet = esm.pretrained.esm1v_t33_650M_UR90S_1()\n",
        "        self.batch_converter = self.alphabet.get_batch_converter()\n",
        "        self.model = self.model.to(DEVICE)\n",
        "\n",
        "\n",
        "    def generate_embedding(self, seq, prot_name):\n",
        "        \"\"\"!  Generate Embedding for a given batch of protein sequences\n",
        "\n",
        "              @param[in]  :   seq - protein sequence as a string\n",
        "              @param[in]  :   prot_name - name of protein sequence\n",
        "\n",
        "              @result     :   save_arr - np sequence embedding\n",
        "        \"\"\"\n",
        "        data = [(prot_name, seq)]\n",
        "        batch_labels, batch_strs, batch_tokens = self.batch_converter(data)\n",
        "        batch_tokens = batch_tokens.to(DEVICE)\n",
        "\n",
        "        ## Extract per-residue representations\n",
        "        with torch.no_grad():\n",
        "          results = self.model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
        "        token_representations = results[\"representations\"][33]\n",
        "\n",
        "        # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "        for i, (prot_n, seq) in enumerate(data):\n",
        "          if DEVICE == \"cpu\":\n",
        "            save_arr = token_representations[i, 1: len(seq) + 1].numpy()\n",
        "          else:\n",
        "            save_arr = token_representations[i, 1: len(seq) + 1].cpu().numpy()\n",
        "\n",
        "        del batch_tokens\n",
        "        return save_arr\n",
        "\n",
        "    def process_dataset(self, file_list):\n",
        "        \"\"\"!  Generate Embeddings for all sequences in a given Dataset\n",
        "\n",
        "              @param[in]  :   file_list - list of protein files for a dataset\n",
        "\n",
        "              @result     :   None\n",
        "        \"\"\"\n",
        "        ## Extract Files from list and itertively process\n",
        "        prot_list = read_list(file_list)\n",
        "        for prot_path in tqdm(prot_list):\n",
        "          prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "          save_path =  os.path.join(self.input_path, prot_name + \"_esm.npy\")\n",
        "          exists = os.path.isfile(save_path)\n",
        "          ## Check no embedding exists\n",
        "          if not exists:\n",
        "            try:\n",
        "              ## Extract Protein Sequence as a String & Process through Model\n",
        "              seq = read_fasta_file(prot_path)\n",
        "              #print(print(\"protein: \", prot_name, \", length: \", len(seq)))\n",
        "              embedding = self.generate_embedding(seq, prot_name)\n",
        "\n",
        "              ## Save np file\n",
        "              np.save(save_path, embedding)\n",
        "            except:\n",
        "              print(\"No file available for: \",  prot_name, prot_path)\n",
        "        return\n",
        "\n",
        "    def load_embeddings(self, protein, seq):\n",
        "        \"\"\"!  Load Embeddings for a given protein\n",
        "              @param[in]  :   protein - name of protein to load\n",
        "              @param[in]  :   seq - protein sequence to generate if not availble\n",
        "\n",
        "              @result     :   np array of embedding\n",
        "        \"\"\"\n",
        "        prot_path =  os.path.join(self.input_path, protein + \"_esm.npy\")\n",
        "        result = None\n",
        "        try:\n",
        "          result = np.load(prot_path)\n",
        "        except:\n",
        "          print(\"No file available for: \",  protein, prot_path)\n",
        "          result = self.generate_embedding(seq, prot_name)\n",
        "\n",
        "          ## Save np file\n",
        "          np.save(prot_path, result)\n",
        "\n",
        "        return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2lLQNQtC1I9",
        "outputId": "c1e5d305-109e-46c0-85ee-3c70a99e95cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/SPOT-1D-LM/spot_1d_lm/fasta\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1542"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import gc\n",
        "print(FASTA_DIR)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKcvrzo-vvc9",
        "outputId": "0cbb88ac-bc18-4cb1-ad78-60cf4d6df3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testfile path:  /content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/SPOT-1D-LM/spot_1d_lm/lists/test2018.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm1v_t33_650M_UR90S_1.pt\" to /root/.cache/torch/hub/checkpoints/esm1v_t33_650M_UR90S_1.pt\n",
            "/usr/local/lib/python3.10/dist-packages/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "test = TEST_FILES[\"TEST-2018\"]\n",
        "print(\"testfile path: \", test)\n",
        "esm_test = PreTrainedESM1b(ESM_CHECKPOINTS_DIR, ESM1B_INPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0x_CNgszdh4",
        "outputId": "09e3f2b5-fa09-48f2-ebbb-abd3b883fff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/38913 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/SPOT-1D-LM/inputs/esm/3TJ3_2_C_esm.npy\n"
          ]
        }
      ],
      "source": [
        "#esm_test.process_dataset(VAL_FILE)\n",
        "prot_list = read_list(TRAIN_FILE)\n",
        "\n",
        "for prot_path in tqdm(prot_list):\n",
        "  prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "  try:\n",
        "    esm_embedding = esm_test.load_embeddings(prot_name)\n",
        "  except:\n",
        "    save_path =  os.path.join(ESM1B_INPUT_DIR, prot_name + \"_esm.npy\")\n",
        "    print(save_path)\n",
        "    seq = read_fasta_file(prot_path)\n",
        "    #print(print(\"protein: \", prot_name, \", length: \", len(seq)))\n",
        "    embedding = generate_embedding(seq, prot_name)\n",
        "\n",
        "    ## Save np file\n",
        "    np.save(save_path, embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-BttZqdybrO"
      },
      "source": [
        "## ProteinBERT\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RJfaY6u4KXu"
      },
      "outputs": [],
      "source": [
        "PB_EMBEDDING_DIM = 1562"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOFGLamUo2ei"
      },
      "outputs": [],
      "source": [
        "class PreTrainedProteinBERT():\n",
        "    \"\"\"!  Pre Trained ProteinBERT Model Class\n",
        "\n",
        "          Used to Access Pre-Trained Keras model and generate embeddings for datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, input_path, max_seq_len=1100):\n",
        "        \"\"\"!  Pre Trained ProteinBERT Model Class initializer\n",
        "\n",
        "              @param[in]  :   model_path = path to saved model checkpoints\n",
        "              @param[in]  :   input_path = path to saved embeddings\n",
        "              @param[in]  :   max_seq_len = maximum protein sequence length\n",
        "        \"\"\"\n",
        "        self.input_path = input_path\n",
        "        self.model_path = model_path\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.pad_value = 25\n",
        "\n",
        "        ## Embedding dim\n",
        "        self.embedding_dim = PB_EMBEDDING_DIM                     ## > NP Embeddings are size = (max_seq_len, embedding_dim)\n",
        "\n",
        "        ## Load Pretrained model and input encoder\n",
        "        self.pretrained_model_generator, self.input_encoder = load_pretrained_model()\n",
        "        ## Lodel model to obtain local_representations & global represntations\n",
        "        self.model = get_model_with_hidden_layers_as_outputs(self.pretrained_model_generator.create_model(self.max_seq_len))\n",
        "\n",
        "\n",
        "    def generate_embedding(self, seq, prot_name):\n",
        "        \"\"\"!  Generate Embedding for a given batch of protein sequences\n",
        "\n",
        "              @param[in]  :   seq - protein sequence as a string\n",
        "              @param[in]  :   prot_name - name of protein sequence\n",
        "\n",
        "              @result     :   save_arr - np sequence embedding\n",
        "        \"\"\"\n",
        "        ## Get raw sequence length\n",
        "        seq_len = len(seq)\n",
        "        one_hot_seq = one_hot(seq)\n",
        "\n",
        "        ## Replace Us with Xs to normalise encoding over models\n",
        "        seq = seq.replace(\"U\", \"X\")\n",
        "\n",
        "\n",
        "        ## Encode Input sequence\n",
        "        encoded_x = self.input_encoder.encode_X([seq], self.max_seq_len)\n",
        "\n",
        "        ## Obtain local & global embeddings\n",
        "        local_representations, global_representations = self.model.predict(encoded_x)\n",
        "        ## Remove padding, end and start tokens\n",
        "        save_arr = local_representations[0,1:seq_len+1,:]\n",
        "\n",
        "        del local_representations, global_representations\n",
        "\n",
        "        return save_arr\n",
        "\n",
        "    def process_dataset(self, file_list):\n",
        "        \"\"\"!  Generate Embeddings for all sequences in a given Dataset\n",
        "\n",
        "              @param[in]  :   file_list - list of protein files for a dataset\n",
        "\n",
        "              @result     :   None\n",
        "        \"\"\"\n",
        "        ## Extract Files from list and itertively process\n",
        "        prot_list = read_list(file_list)\n",
        "        counter = 0\n",
        "        for prot_path in tqdm(prot_list):\n",
        "          prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "          save_path =  os.path.join(self.input_path, prot_name + \"_pb.npy\")\n",
        "          exists = os.path.isfile(save_path)\n",
        "          print(exists)\n",
        "          try:\n",
        "            seq = read_fasta_file(prot_path)\n",
        "            embedding = self.generate_embedding(seq, prot_name)\n",
        "          except:\n",
        "            pass\n",
        "\n",
        "          ## Check no embedding exists\n",
        "          if not exists:\n",
        "            try:\n",
        "              ## Extract Protein Sequence as a String & Process through Model\n",
        "              seq = read_fasta_file(prot_path)\n",
        "              embedding = self.generate_embedding(seq, prot_name)\n",
        "\n",
        "              ## Save np file\n",
        "              np.save(save_path, embedding)\n",
        "            except:\n",
        "              print(\"No file available for: \",  prot_name, prot_path)\n",
        "        return\n",
        "\n",
        "    def load_embeddings(self, protein):\n",
        "        \"\"\"!  Load Embeddings for a given protein\n",
        "              @param[in]  :   protein - name of protein to load\n",
        "\n",
        "              @result     :   np array of embedding\n",
        "        \"\"\"\n",
        "        prot_path =  os.path.join(self.input_path, protein + \"_pb.npy\")\n",
        "        result = None\n",
        "        try:\n",
        "          result = np.load(prot_path)\n",
        "        except:\n",
        "          print(\"No file available for: \",  protein, prot_path)\n",
        "          result = self.generate_embedding(seq, prot_name)\n",
        "\n",
        "          ## Save np file\n",
        "          np.save(prot_path, result)\n",
        "\n",
        "        return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4__psrR3ULB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88aa3c40-66a6-45a9-bd7a-9951d8787e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Local model dump file /root/proteinbert_models/default.pkl doesn't exist. Will download ftp://ftp.cs.huji.ac.il/users/nadavb/protein_bert/epoch_92400_sample_23500000.pkl into /root/proteinbert_models. Please approve or reject this (to exit and potentially call the function again with different parameters).\n",
            "Do you approve downloading the file into the specified directory? Please specify \"Yes\" or \"No\":Yes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "URLError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'I'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mconnect_ftp\u001b[0;34m(self, user, passwd, host, port, dirs, timeout)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m         return ftpwrapper(user, passwd, host, port, dirs, timeout,\n\u001b[0m\u001b[1;32m   1586\u001b[0m                           persistent=False)\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user, passwd, host, port, dirs, timeout, persistent)\u001b[0m\n\u001b[1;32m   2405\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mftplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFTP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasswd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ftplib.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, host, port, timeout, source_address)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ftplib.connect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         self.sock = socket.create_connection((self.host, self.port), self.timeout,\n\u001b[0m\u001b[1;32m    159\u001b[0m                                              source_address=self.source_address)\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-8f1d3d41ce27>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprotein\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'3S9E_1_A'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpb_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedProteinBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPB_CHECKPOINTS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPB_INPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#seq = read_fasta_file(prot_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-8f0aea3f434d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, input_path, max_seq_len)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m## Load Pretrained model and input encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m## Lodel model to obtain local_representations & global represntations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_with_hidden_layers_as_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/ProteinBERT/proteinbert_keras/proteinbert/existing_model_loading.py\u001b[0m in \u001b[0;36mload_pretrained_model\u001b[0;34m(local_model_dump_dir, local_model_dump_file_name, remote_model_dump_url, download_model_dump_if_not_exists, validate_downloading, create_model_function, create_model_kwargs, optimizer_class, lr, other_optimizer_kwargs, annots_loss_weight, load_optimizer_weights)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloaded_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cannot download into an already existing file: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdownloaded_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_model_dump_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mremote_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloaded_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlocal_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0maddinfourl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mftplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 111] Connection refused>"
          ]
        }
      ],
      "source": [
        "protein = '3S9E_1_A'\n",
        "pb_test = PreTrainedProteinBERT(PB_CHECKPOINTS_DIR, PB_INPUT_DIR)\n",
        "#seq = read_fasta_file(prot_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k7EYClL0U8W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "446c9746-c0a0-4f4f-eb86-60350512a6ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6b0e952d5526>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPB_INPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprot_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_pb.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'prot_name' is not defined"
          ]
        }
      ],
      "source": [
        "save_path =  os.path.join(PB_INPUT_DIR, prot_name + \"_pb.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD00QIbC217S",
        "outputId": "0ca2991f-a2ea-4a9e-db51-57e29814dfde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/99 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No pb file available for:  4GQZ_1_A 4GQZ_1_A\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 99/99 [05:47<00:00,  3.51s/it]\n"
          ]
        }
      ],
      "source": [
        "#pb_test.process_dataset(VAL_FILE)\n",
        "prot_list = read_list(VAL_FILE)\n",
        "\n",
        "for prot_path in tqdm(prot_list):\n",
        "  prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "  try:\n",
        "    pb_embedding = pb_test.load_embeddings(prot_name)\n",
        "  except:\n",
        "    print(\"No pb file available for: \",  prot_name, prot_path)\n",
        "  try:\n",
        "    esm_embedding = esm_test.load_embeddings(prot_name)\n",
        "  except:\n",
        "    print(\"No esm file available for: \",  prot_name, prot_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzoClQxPvJCV"
      },
      "source": [
        "4V6W_57_CY 4V6W_57_CY\n",
        "  3%|▎         | 1212/38913 [05:51<5:28:59,  1.91it/s]No esm file available for:  3WU2_12_M 3WU2_12_M\n",
        "  4%|▎         | 1409/38913 [07:50<11:17:13,  1.08s/it]No pb file available for:  3S9E_1_A 3S9E_1_A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQE-uwljESye"
      },
      "source": [
        "## ProtTrans\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_ICMQjc4TM9"
      },
      "outputs": [],
      "source": [
        "PT_EMBEDDING_DIM = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVzkrqqiEWYK"
      },
      "outputs": [],
      "source": [
        "class PreTrainedProtTrans():\n",
        "    \"\"\"!  Pre Trained ProtTrans Model Class\n",
        "\n",
        "          Used to Access Pre-Trained model and generate embeddings for datasets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, input_path):\n",
        "        \"\"\"!  Pre Trained ProtTrans Model Class initializer\n",
        "\n",
        "              @param[in]  :   model_path = path to saved model checkpoints\n",
        "              @param[in]  :   input_path = path to saved embeddings\n",
        "        \"\"\"\n",
        "        self.input_path = input_path\n",
        "\n",
        "        self.embedding_dim = PT_EMBEDDING_DIM\n",
        "\n",
        "        ## loads tokenizer and model for prot_t5_xl_uniref50\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
        "        self.model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
        "        self.model = self.model.to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "    def generate_embedding(self, seq, prot_name):\n",
        "        \"\"\"!  Generate Embedding for a given batch of protein sequences\n",
        "\n",
        "              @param[in]  :   seq - protein sequence as a string\n",
        "              @param[in]  :   prot_name - name of protein sequence\n",
        "\n",
        "              @result     :   save_arr - np sequence embedding\n",
        "        \"\"\"\n",
        "        ## Insert spaces between each character in the sequence\n",
        "        seq_temp = seq.replace('', \" \")\n",
        "        ## Create a list of sequences\n",
        "        sequences_Example = [seq_temp]\n",
        "        ## Replace certain characters with 'X' for normalization\n",
        "        sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]\n",
        "        ## Tokenize the sequence\n",
        "        ids = self.tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True)\n",
        "\n",
        "        ## convert the tokenized sequences to tensors and move to device\n",
        "        input_ids = torch.tensor(ids['input_ids']).to(DEVICE)\n",
        "        attention_mask = torch.tensor(ids['attention_mask']).to(DEVICE)\n",
        "        ## Generate embeddings with no gradient calculations (for efficiency)\n",
        "        with torch.no_grad():\n",
        "          embedding = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        ## move embeddings to cpu if cuda and convert to numpy array\n",
        "        if DEVICE == \"cpu\":\n",
        "          embedding = embedding.last_hidden_state.numpy()\n",
        "        else:\n",
        "          embedding = embedding.last_hidden_state.cpu().numpy()\n",
        "\n",
        "        ## extract and process embeddings for each sequence\n",
        "        features = []\n",
        "        for seq_num in range(len(embedding)):\n",
        "          ## calculate the actual sequence length (excluding padding)\n",
        "          seq_len = (attention_mask[seq_num] == 1).sum()\n",
        "          ## extract the embeddings for the actual sequence length\n",
        "          seq_emd = embedding[seq_num][:seq_len - 1]\n",
        "          features.append(seq_emd)\n",
        "\n",
        "        save_arr = features[0]\n",
        "\n",
        "        del input_ids, attention_mask\n",
        "        return save_arr\n",
        "\n",
        "\n",
        "    def process_dataset(self, file_list):\n",
        "        \"\"\"!  Generate Embeddings for all sequences in a given Dataset\n",
        "\n",
        "              @param[in]  :   file_list - list of protein files for a dataset\n",
        "\n",
        "              @result     :   None\n",
        "        \"\"\"\n",
        "        ## Set model to evaluation mode to disable dropout for deterministic results\n",
        "        self.model.eval()\n",
        "\n",
        "        ## Extract Files from list and itertively process\n",
        "        prot_list = read_list(file_list)\n",
        "        for prot_path in tqdm(prot_list):\n",
        "          prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "          save_path =  os.path.join(self.input_path, prot_name + \"_pt.npy\")\n",
        "          print(\"protein: \", prot_name, prot_path)\n",
        "          exists = os.path.isfile(save_path)\n",
        "          print(\"exists: \", exists)\n",
        "          ## Check no embedding exists\n",
        "          if not exists:\n",
        "            ## Extract Protein Sequence as a String & Process through Model\n",
        "            try:\n",
        "              seq = read_fasta_file(prot_path)\n",
        "              embedding = self.generate_embedding(seq, prot_name)\n",
        "\n",
        "              ## Save np file\n",
        "              print(save_path)\n",
        "              np.save(save_path, embedding)\n",
        "              print(os.path.isfile(save_path))\n",
        "            except:\n",
        "              print(\"No file available for: \",  prot_name, prot_path)\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "Wokm-9BKbwkE",
        "outputId": "93fded0d-297e-44d3-96ce-b345c5d06eb4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-789e29271792>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpt_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedProtTrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mESM_CHECKPOINTS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPT_INPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'PreTrainedProtTrans' is not defined"
          ]
        }
      ],
      "source": [
        "pt_test = PreTrainedProtTrans(ESM_CHECKPOINTS_DIR, PT_INPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGwnQO0Xb14L"
      },
      "outputs": [],
      "source": [
        "pt_test.process_dataset(TRAIN_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEC5UmlNcTsn"
      },
      "source": [
        "# Datasets\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGlGfyICf0QJ"
      },
      "source": [
        "## Helper Functions & Utilities\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esb5kxVugDQN"
      },
      "source": [
        "### **File I/O Functions for Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg1A_WZqf30P"
      },
      "outputs": [],
      "source": [
        "def read_list(file_name):\n",
        "    \"\"\"!  Read a text file to get the list of elements\n",
        "\n",
        "          @param[in]  :   file_name - complete path to a file (string)\n",
        "          @return     :   list of elements in the text file\n",
        "    \"\"\"\n",
        "    with open(file_name, 'r') as f:\n",
        "        text = f.read().splitlines()\n",
        "    return text\n",
        "\n",
        "\n",
        "def read_fasta_file(fname):\n",
        "    \"\"\"! Reads the sequence from the fasta file\n",
        "\n",
        "         @param[in]  :   fname - complete path to a fasta file (string)\n",
        "         @return     :   protein sequence (string)\n",
        "    \"\"\"\n",
        "    fname = os.path.join(FASTA_DIR, fname+\".fasta\")\n",
        "    #print(fname, os.path.isfile(fname))\n",
        "    with open(fname, 'r') as f:\n",
        "        AA = ''.join(f.read().splitlines()[1:])\n",
        "    return AA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJW57jdenCp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dataset_csv(file_name_list, df_file_name):\n",
        "    \"\"\"!  for a given dataset format all proteins into a csv containing sequence, sequence length & protein name\n",
        "\n",
        "          @param[in]  :   file_name_list - complete path to a file (string)\n",
        "          @param[in]  :   df_file_name - complete path to save output csv (string)\n",
        "\n",
        "          @return     :   dataframe containing sequence, sequence length & protein name\n",
        "    \"\"\"\n",
        "    protein_file_list = read_list(file_name_list)\n",
        "    ## Create Data frame to store Data\n",
        "    df = data_frame = pd.DataFrame(columns=['Protein', 'Sequence', 'Length'])\n",
        "\n",
        "    ## Interate through protein list and extract data\n",
        "    for prot_path in tqdm(protein_file_list):\n",
        "      try:\n",
        "          ## file path for the protein at index idx\n",
        "          ## extracts the protein name from the protein path\n",
        "          protein = prot_path.split('/')[-1].split('.')[0]\n",
        "          \"\"\"!  @brief  :  Determine Protein Sequence & length \"\"\"\n",
        "          seq = read_fasta_file(prot_path)\n",
        "          protein_len = len(seq)\n",
        "\n",
        "          \"\"\"!  @brief  :   Append to Dataframe \"\"\"\n",
        "          df.loc[len(df)] = {'Protein':protein, 'Sequence': seq, 'Length':protein_len}\n",
        "\n",
        "      except:\n",
        "          print(\"no file for \", protein)\n",
        "\n",
        "    ## save as csv\n",
        "    df.to_csv(df_file_name, sep='\\t')\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3uMluoXirN5"
      },
      "outputs": [],
      "source": [
        "#df_val = dataset_csv(TRAIN_FILE, TRAIN_DF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBC89jcrnvw1"
      },
      "outputs": [],
      "source": [
        "#pan = pd.read_csv(TRAIN_DF, sep='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bZbBYsSoBr9"
      },
      "outputs": [],
      "source": [
        "#print(pan['Protein'].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GympS3xPD3tZ"
      },
      "source": [
        "### **File I/O Functions for Label files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAbqsE1zDlKn"
      },
      "outputs": [],
      "source": [
        "def get_dssp_info(dssp_file_name):\n",
        "    \"\"\"!  Parse DSSP file to obtain label info\n",
        "\n",
        "          @param[in]  :   dssp_file_name - includes file path and extension\n",
        "          @return     :   dssp - dssp as pd dataframe\n",
        "    \"\"\"\n",
        "    with open(dssp_file_name, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    # end with\n",
        "\n",
        "    # Extracting the relevant lines\n",
        "    amino_acid_code = lines[1].strip()\n",
        "    ss3 = lines[2].strip()\n",
        "    phi_angles = lines[3].strip().split()\n",
        "    psi_angles = lines[4].strip().split()\n",
        "    asa = lines[5].strip().split()\n",
        "\n",
        "    # Creating the dataframe\n",
        "    dssp = pd.DataFrame({\n",
        "        'AA CODE': list(amino_acid_code),\n",
        "        'SS3': list(ss3),\n",
        "        'PHI': phi_angles,\n",
        "        'PSI': psi_angles,\n",
        "        'ASA': asa\n",
        "    })\n",
        "\n",
        "    # this converts to float numbers and accounts for 'X'\n",
        "    # converts missing 'X' to Nan\n",
        "    dssp['PHI'] = pd.to_numeric(dssp['PHI'], errors='coerce')\n",
        "    dssp['PSI'] = pd.to_numeric(dssp['PSI'], errors='coerce')\n",
        "    dssp['ASA'] = pd.to_numeric(dssp['ASA'], errors='coerce')\n",
        "    return dssp\n",
        "# end def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1saxMiAQEKkL"
      },
      "outputs": [],
      "source": [
        "def get_theta_info(theta_file_name):\n",
        "    \"\"\"!  Parse theta file to obtain label info\n",
        "\n",
        "          @param[in]  :   theta_file_name i- ncludes file path and extension\n",
        "          @return     :   theta - theta as pd dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    columns = ['RES NUM', 'AA CODE', 'THETA', 'TAU', 'OMEGA']\n",
        "    # creates data frame from file\n",
        "    theta = pd.read_csv(theta_file_name, sep=' ', names=columns)\n",
        "\n",
        "    # this converts to float or ints and accounts for 'X' to NaN\n",
        "    theta['THETA']  = pd.to_numeric(theta['THETA'], errors='coerce')\n",
        "    theta['TAU']    = pd.to_numeric(theta['TAU'], errors='coerce')\n",
        "    theta['OMEGA']  = pd.to_numeric(theta['OMEGA'], errors='coerce')\n",
        "\n",
        "    return theta\n",
        "# end def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLU_7pKZEjeJ"
      },
      "outputs": [],
      "source": [
        "def get_hse_info(hse_file_name, CASP=False):\n",
        "    \"\"\"!  Parse hse file to obtain label info\n",
        "\n",
        "          @param[in]  :   hse_file_name - includes file path and extension\n",
        "          @return     :   hse - hse as pd dataframe\n",
        "    \"\"\"\n",
        "    columns = ['AA NAME', 'CHAIN ID', 'RES NUM', 'AA CODE',\n",
        "               'HSE TOTAL', 'HSE UP', 'HSE DOWN']\n",
        "\n",
        "    # Attempt to read the file\n",
        "    hse = pd.read_csv(hse_file_name, sep=r'\\s+', names=columns)\n",
        "\n",
        "    # this converts to float or ints and accounts for 'X' to NaN\n",
        "    hse['CHAIN ID'] = pd.to_numeric(hse['CHAIN ID'], errors='coerce')\n",
        "    hse['RES NUM'] = pd.to_numeric(hse['RES NUM'], errors='coerce')\n",
        "    hse['HSE TOTAL'] = pd.to_numeric(hse['HSE TOTAL'], errors='coerce')\n",
        "    hse['HSE UP'] = pd.to_numeric(hse['HSE UP'], errors='coerce')\n",
        "    hse['HSE DOWN'] = pd.to_numeric(hse['HSE DOWN'], errors='coerce')\n",
        "    # print(hse.head)\n",
        "\n",
        "    return hse\n",
        "# end def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bZf_zmRgjku"
      },
      "source": [
        "## Input Data Types\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CTkjCZJgx7t"
      },
      "source": [
        "### **Protein Sequence 1-hot Enconding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tvxxCxsgxN2"
      },
      "outputs": [],
      "source": [
        "def one_hot(seq):\n",
        "    \"\"\"!  Converts a sequence to one hot encoding\n",
        "          @param[in]  :   seq - amino acid sequence (string)\n",
        "          @return     :   one hot encoding of the amino acid (array)[L,20]\n",
        "    \"\"\"\n",
        "    prot_seq = seq\n",
        "    BASES = 'ARNDCQEGHILKMFPSTWYV'\n",
        "    bases = np.array([base for base in BASES])\n",
        "    feat = np.concatenate(\n",
        "        [[(bases == base.upper()).astype(int)] if str(base).upper() in BASES else np.array([[-1] * len(BASES)]) for base\n",
        "         in prot_seq])\n",
        "    return feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffrThYvh7toU"
      },
      "outputs": [],
      "source": [
        "def generate_sequence_embeddings(file_list):\n",
        "    \"\"\"!  Generate amino acid one hot embeddings for a given dataset\n",
        "          @param[in]  :   file_list - list of protein files for a dataset\n",
        "          @return     :   one hot encoding of the amino acid (array)[L,20]\n",
        "    \"\"\"\n",
        "    ## Extract Files from list and itertively process\n",
        "    prot_list = read_list(file_list)\n",
        "\n",
        "    for prot_path in tqdm(prot_list):\n",
        "        prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "        save_path =  os.path.join(ONEHOT_INPUT_DIR, prot_name + \"_1hot.npy\")\n",
        "        exists = os.path.isfile(save_path)\n",
        "        print(\"exts=\", exists)\n",
        "        ## Check no embedding exists\n",
        "        if not exists:\n",
        "          ## Extract Protein Sequence as a String\n",
        "          seq = read_fasta_file(prot_path)\n",
        "          print(\"protein: \", prot_name, \", length: \", len(seq))\n",
        "\n",
        "          ## Generate 1 hot embedding\n",
        "          embedding = one_hot(seq)\n",
        "\n",
        "          ## Save np file\n",
        "          #print(save_path)\n",
        "          np.save(save_path, embedding)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqWqM3LJ-i3U"
      },
      "outputs": [],
      "source": [
        "#generate_sequence_embeddings(VAL_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNM60a8_AHFs"
      },
      "outputs": [],
      "source": [
        "#np.load('/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/SPOT-1D-LM/inputs/one_hot/3TJ3_2_C_1hot.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc_VZMDNAHNb"
      },
      "outputs": [],
      "source": [
        "##  @brief  : Check for missing data\n",
        "prot_list = TRAIN_FILE\n",
        "#for prot_path in tqdm(prot_list):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGkyU_wZhOfm"
      },
      "source": [
        "## Output Data Types\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAe6Mdgzme0T"
      },
      "source": [
        "### **3 State Secondary Structure (SS3) - Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4RYJUBNnH0A"
      },
      "source": [
        "Variable Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e2x--SMmeff"
      },
      "outputs": [],
      "source": [
        "##  @brief  : Class Definitions for SS3 Assignment\n",
        "##            -> C = Coil Properties\n",
        "##            -> E = Beta Strand Properties\n",
        "##            -> H = Alpha Helix Properties\n",
        "SS3_CLASSES = ['C', 'E', 'H']\n",
        "\n",
        "##  @brief  : Class Definitions for SS8 Assignment\n",
        "##            Coil Properties\n",
        "##               -> C = Irregular Coil / Other\n",
        "##               -> S = High Curvature Loop\n",
        "##               -> T = Beta-Turn\n",
        "##            Alpha Helix Properties\n",
        "##               -> H = Alpha Helix\n",
        "##               -> G = 310 Helix\n",
        "##               -> I = Pi Helix\n",
        "##            Beta Strand Properties\n",
        "##               -> E = Beta Strand\n",
        "##               -> B = Beta Bridge\n",
        "SS8_CLASSES = ['C', 'S', 'T', 'H', 'G', 'I', 'E', 'B']\n",
        "\n",
        "##  @brief  : Dictionary Mapping SS8 Classes to SS3 Classes\n",
        "##            -> We choose to use SS3 only as it provides a simplification of the SS8 information\n",
        "##            -> Reduce Complexity and Memory of Model by simplifying features\n",
        "ss_conv_3_8_dict = {'X': 'X', 'C': 'C', 'S': 'C', 'T': 'C', 'H': 'H', 'G': 'H', 'I': 'H', 'E': 'E', 'B': 'E'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viGfAMUQhW1Q"
      },
      "source": [
        "### **Accessible Surface Area (ASA) - Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wNCDQiqh6Yc"
      },
      "source": [
        "Variable Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulkpB4jeh9w4"
      },
      "outputs": [],
      "source": [
        "##  @brief  : Standard ASA values for each amino acid\n",
        "ASA_std = {\"A\": 115, \"C\": 135, \"D\": 150, \"E\": 190, \"F\": 210, \"G\": 75, \"H\": 195,\n",
        "               \"I\": 175, \"K\": 200, \"L\": 170, \"M\": 185, \"N\": 160, \"P\": 145, \"Q\": 180,\n",
        "               \"R\": 225, \"S\": 115, \"T\": 140, \"V\": 155, \"W\": 255, \"Y\": 230, \"-\": 1, \"X\": 1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZPcP4jtiB5x"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hek9kHlhusx"
      },
      "outputs": [],
      "source": [
        "def normalize_asa(asa_values, amino_acid_seq):\n",
        "    \"\"\"!  Normalize ASA values for given amino acid sequence\n",
        "\n",
        "          @param[in]  :   asa_values - asa model output values\n",
        "          @param[in]  :   amino_acid_seq - corresponding protein sequence\n",
        "\n",
        "          @return     :   normalized_asa - normalized asa values\n",
        "    \"\"\"\n",
        "    ASA_std = {\"A\": 115, \"C\": 135, \"D\": 150, \"E\": 190, \"F\": 210, \"G\": 75, \"H\": 195,\n",
        "               \"I\": 175, \"K\": 200, \"L\": 170, \"M\": 185, \"N\": 160, \"P\": 145, \"Q\": 180,\n",
        "               \"R\": 225, \"S\": 115, \"T\": 140, \"V\": 155, \"W\": 255, \"Y\": 230, \"-\": 1, \"X\": 1}\n",
        "\n",
        "    normalized_asa = []\n",
        "    for asa, aa in zip(asa_values, amino_acid_seq):\n",
        "        max_asa = ASA_std.get(aa, 1)  # Default to 1 for unknown amino acids\n",
        "        norm_asa = asa / max_asa  # Normalize the ASA value\n",
        "        normalized_asa.append(norm_asa)\n",
        "\n",
        "    return normalized_asa\n",
        "\n",
        "\n",
        "def unnormalize_asa(normalized_asa, amino_acid_seq):\n",
        "    \"\"\"!  Unnormalize ASA values for given amino acid sequence\n",
        "\n",
        "          @param[in]  :   normalized_asa - normalized asa values\n",
        "          @param[in]  :   amino_acid_seq - corresponding protein sequence\n",
        "\n",
        "          @return     :   unnormalized_asa - asa rawoutput values\n",
        "    \"\"\"\n",
        "    unnormalized_asa = []\n",
        "    for norm_asa, aa in zip(normalized_asa, amino_acid_seq):\n",
        "        max_asa = ASA_std.get(aa, 1)  # Default to 1 for unknown amino acids\n",
        "        abs_asa = norm_asa * max_asa  # Un-normalize the ASA value\n",
        "        unnormalized_asa.append(abs_asa)\n",
        "\n",
        "    return unnormalized_asa\n",
        "\n",
        "\n",
        "def get_unnorm_asa_new(rel_asa, seq):\n",
        "    \"\"\"!  calculates absolute ASA from relative ASA\n",
        "          uses standard ASA values for amino acids and computes the absolute ASA based on the\n",
        "          sequence and prediced relative ASA\n",
        "\n",
        "          @param[in]  :   asa_pred - The predicted relative ASA\n",
        "          @param[in]  :   seq_list: Sequence of the protein\n",
        "\n",
        "          @return     :   absolute ASA_PRED\n",
        "    \"\"\"\n",
        "\n",
        "    ## defines a string of standard amino acid one letter\n",
        "    ## codes plus a symbol for unknown X\n",
        "    rnam1_std = \"ACDEFGHIKLMNPQRSTVWY-X\"\n",
        "\n",
        "    ## tuple containing standard ASA for each AA in rnam1_std\n",
        "    ASA_std = (115, 135, 150, 190, 210, 75, 195, 175, 200, 170,\n",
        "               185, 160, 145, 180, 225, 115, 140, 155, 255, 230, 1, 1)\n",
        "    ## creates dictionary mapping each AA to its standard ASA\n",
        "    dict_rnam1_ASA = dict(zip(rnam1_std, ASA_std))\n",
        "\n",
        "    ## processing each sequence in the batch\n",
        "    ## the length of the first sequence in the batch\n",
        "    ## assuming all sequences are of equal length or\n",
        "    ## padded to same length\n",
        "    max_seq_len = len(seq[0])\n",
        "    array_list = [] # stores absolute ASA\n",
        "    for i, single_seq in enumerate(list(seq)):\n",
        "        ## gets relative ASA predictions for the current sequence\n",
        "        rel_asa_current = rel_asa[i, :]\n",
        "        ## calculates the difference b/w the max sequence length and the\n",
        "        ## current sequence\n",
        "        seq_len_diff = max_seq_len - len(single_seq)\n",
        "        ## pads the current single sequence with X to match the\n",
        "        ## max sequence length\n",
        "        single_seq = single_seq + (\"X\" * seq_len_diff)\n",
        "        ## creates an array of standard ASA values corresponding to each AA in padded\n",
        "        ## sequence\n",
        "        asa_max = np.array([dict_rnam1_ASA[i] for i in single_seq]).astype(np.float32)\n",
        "        ## multiplies the relative ASA predictions with the standard ASA values to get\n",
        "        ## absolute ASA values\n",
        "        abs_asa = np.multiply(rel_asa_current.cpu().detach().numpy(), asa_max)\n",
        "        array_list.append(abs_asa)\n",
        "\n",
        "    final_array = np.array(array_list)\n",
        "    return final_array\n",
        "# end def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRYHxv1kBDu"
      },
      "source": [
        "### **Half Sphere Exposure (HSE) - Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llDr5QnmlQVp"
      },
      "source": [
        "Variable Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CFMnq2slQBo"
      },
      "outputs": [],
      "source": [
        "max_hseu = 50                           ## > Maximum HSE-U Value\n",
        "max_hsed = 65                           ## > Maximum HSE-D Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frHPeHH9kKmq"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6nnf5HRkKHd"
      },
      "outputs": [],
      "source": [
        "##  @brief  : HSE-u\n",
        "def normalize_hseu(hseu_values):\n",
        "    \"\"\"!  Normalize HSE-U values by dividing by the maximum HSE-U value (50)\n",
        "\n",
        "          @param[in]  :   hseu_values - HSE-U values for a protein.\n",
        "\n",
        "          @return     :   normalized_hseu - normalized HSE-U values.\n",
        "    \"\"\"\n",
        "    return hseu_values / max_hseu\n",
        "\n",
        "def unnormalize_hseu(normalized_hseu_values):\n",
        "    \"\"\"!  Unnormalize HSE-U values by multiplying by the maximum HSE-U value (50).\n",
        "\n",
        "          @param[in]  :   normalized_hseu_values - Normalized HSE-U values.\n",
        "\n",
        "          @return     :   hseu_ - Original HSE-U values.\n",
        "    \"\"\"\n",
        "    return normalized_hseu_values * max_hseu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HyyMZfbkwQB"
      },
      "outputs": [],
      "source": [
        "##  @brief  : HSE-d\n",
        "def normalize_hsed(hsed_values):\n",
        "    \"\"\"!  Normalize HSE-D values by dividing by the maximum HSE-D value (65)\n",
        "\n",
        "          @param[in]  :   hsed_values - HSE-D values for a protein.\n",
        "\n",
        "          @return     :   normalized_hsed - normalized HSE-D values.\n",
        "    \"\"\"\n",
        "    return hsed_values / max_hsed\n",
        "\n",
        "def unnormalize_hseu(normalized_hsed_values):\n",
        "    \"\"\"!  Unnormalize HSE-D values by multiplying by the maximum HSE-D value (65).\n",
        "\n",
        "          @param[in]  :   normalized_hsed_values - Normalized HSE-D values.\n",
        "\n",
        "          @return     :   hsed_ - Original HSE-D values.\n",
        "    \"\"\"\n",
        "    return normalized_hsed_values * max_hsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s81JxzcQlnoR"
      },
      "source": [
        "### **Protein Backbone Difedral Angles - Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rKKQdJOlsq7"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waQyYdt4lxli"
      },
      "outputs": [],
      "source": [
        "def normalize_circular_angles(angles):\n",
        "    \"\"\"!  Normalize angle measurements by converting to sine and cosine components and concatenating them.\n",
        "\n",
        "          @param[in]  :    angles (list or numpy array): Angles in degrees.\n",
        "\n",
        "          @return     :    concatenated_angles - numpy array: A 2D array with sine and cosine components concatenated.\n",
        "    \"\"\"\n",
        "    angles_rad = np.radians(angles)  # Convert angles from degrees to radians\n",
        "    angles_sin = np.sin(angles_rad)  # Sine component [-1,1]\n",
        "    angles_cos = np.cos(angles_rad)  # Cosine component [-1,1]\n",
        "\n",
        "    # scale from [-1,1] to [0,1]\n",
        "    angles_sin = (angles_sin + 1)/2\n",
        "    angles_cos = (angles_cos + 1)/2\n",
        "\n",
        "    concatenated_angles = np.column_stack((angles_sin, angles_cos))\n",
        "    return concatenated_angles\n",
        "# end def\n",
        "\n",
        "def unnormalize_circular_angles(normalized_angles):\n",
        "    \"\"\"!  Unnormalize angles from their sine and cosine components.\n",
        "\n",
        "          @param[in]  :    normalized_angles (numpy array): A 2D array with sine and cosine components concatenated.\n",
        "\n",
        "          @return     :    original_angles_deg (numpy array): The original angles in degrees.\n",
        "    \"\"\"\n",
        "    # Extract sine and cosine components\n",
        "    # Sclae from [0,1] back to [-1,1]\n",
        "    angles_sin = normalized_angles[:, 0] * 2 - 1\n",
        "    angles_cos = normalized_angles[:, 1] * 2 - 1\n",
        "    original_angles_rad = np.arctan2(angles_sin, angles_cos)  # Compute the angles in radians\n",
        "    original_angles_deg = np.degrees(original_angles_rad)  # Convert radians to degrees\n",
        "    original_angles_deg = np.mod(original_angles_deg, 360)  # Normalize angles to be within [0, 360] range\n",
        "    return original_angles_deg\n",
        "# end def"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSh-CK0E42G"
      },
      "source": [
        "### **Generate / Format Label Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba-_gObPt-MK"
      },
      "outputs": [],
      "source": [
        "def convert_data_to_numpy(file_list):\n",
        "    \"\"\"!  Format all Label Data into a single numpy array & save\n",
        "\n",
        "          @param[in]  :    file_list - list of protein files for a dataset\n",
        "\n",
        "          @return     :    original_angles_deg (numpy array): The original angles in degrees.\n",
        "    \"\"\"\n",
        "    ## Extensions for label file types\n",
        "    dssp_ext = '.dssp'\n",
        "    hse_ext = '.h'\n",
        "    theta_ext = '.t'\n",
        "    numpy_ext = '.npy'\n",
        "\n",
        "    prot_list = read_list(file_list)\n",
        "\n",
        "    for prot_path in tqdm(prot_list):\n",
        "        prot_name = prot_path.split('/')[-1].split('.')[0]\n",
        "        save_path = os.path.join(LABEL_DIR, prot_name + numpy_ext)\n",
        "        if not os.path.exists(save_path):\n",
        "\n",
        "            ## Get DSSP for protein\n",
        "            dssp_path = os.path.join(DSSP_DIR, prot_name + dssp_ext)\n",
        "            if not os.path.exists(dssp_path):\n",
        "                continue\n",
        "            dssp_data = get_dssp_info(dssp_path)\n",
        "\n",
        "            ## Get HSE for protein\n",
        "            hse_path = os.path.join(HSE_DIR, prot_name + hse_ext)\n",
        "            if not os.path.exists(hse_path):\n",
        "                continue\n",
        "            hse_data = get_hse_info(hse_path)\n",
        "\n",
        "            ## Get Theta for protein\n",
        "            theta_path = os.path.join(THETA_DIR, prot_name + theta_ext)\n",
        "            if not os.path.exists(theta_path):\n",
        "                continue\n",
        "            theta_data = get_hse_info(theta_path)\n",
        "\n",
        "            ## Merge Dataframes\n",
        "            hse_dssp = pd.merge(hse_data, dssp_data,\n",
        "                            how='inner',\n",
        "                            suffixes=('', '_remove'),\n",
        "                            left_index=True,\n",
        "                            right_index=True)\n",
        "            hse_dssp.drop([i for i in hse_dssp.columns if 'remove' in i], axis=1, inplace=True)\n",
        "\n",
        "            protein_data = pd.merge(hse_dssp, theta_data,\n",
        "                                how='inner',\n",
        "                                suffixes=('', '_remove'),\n",
        "                                left_index=True,\n",
        "                                right_index=True)\n",
        "\n",
        "            protein_data.drop([i for i in protein_data.columns if 'remove' in i], axis=1, inplace=True)\n",
        "\n",
        "            ## Reorder columns\n",
        "            desired_order = ['AA NAME', 'CHAIN ID', 'RES NUM', 'AA CODE', 'SS3', 'ASA', 'HSE TOTAL', 'HSE UP', 'HSE DOWN', 'PHI', 'PSI', 'THETA', 'TAU', 'OMEGA']\n",
        "\n",
        "            ## Check for missing data\n",
        "            if len(desired_order) != len(protein_data.columns):\n",
        "              for col in desired_order:\n",
        "                if col not in protein_data.columns:\n",
        "                  protein_data[col] = np.nan\n",
        "\n",
        "            protein_data = protein_data[desired_order]\n",
        "\n",
        "            np.save(save_path, protein_data.to_numpy())\n",
        "            print(f'{prot_name} file generated')\n",
        "        else:\n",
        "          print(f'{prot_name} already converted')\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLgyW9aVo_2z"
      },
      "source": [
        "## Dataset Class Definitions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ_qchjrcwlS"
      },
      "source": [
        "### **Classification Inference Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Flp0gtQXqKhR"
      },
      "outputs": [],
      "source": [
        "class Proteins_Dataset_Class(Dataset):\n",
        "    \"\"\"!  Protein Dataset Classification Class\n",
        "\n",
        "          Dataset of Protein Sequences & learned Regression Features\n",
        "\n",
        "          Loads information from File List\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name_list, file_df, pre_trained_model1, pre_trained_model2):\n",
        "        \"\"\"!  Protein Dataset Classification class initializer\n",
        "\n",
        "              @param[in]  :   file_name_list - List of paths to protein sequence fasta files\n",
        "              @param[in]  :   file_df - Dataframe of sequences\n",
        "              @param[in]  :   pre_trained_model1 - Object to generate / process embeddings from pre-trained model 1\n",
        "              @param[in]  :   pre_trained_model2 - Object to generate / process embeddings from pre-trained model 2\n",
        "        \"\"\"\n",
        "        self.protein_file_list = read_list(file_name_list)       ## > list of file paths to fasta per protein\n",
        "        self.pre_trained_model1 = pre_trained_model1\n",
        "        self.pre_trained_model2 = pre_trained_model2\n",
        "        self.features = []\n",
        "        self.labels = []\n",
        "\n",
        "\n",
        "        ## Store File name lists & Sequences\n",
        "        self.df = pd.read_csv(file_df, sep='\\t')\n",
        "        self.protein_list = self.df['Protein'].to_numpy()\n",
        "        self.sequence_list = self.df['Sequence'].to_numpy()\n",
        "        self.protein_len_list = self.df['Length'].to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"!  Determine number of Data points in dataset\n",
        "\n",
        "              @return   :   length = length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.protein_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"!  Return protein in dataset corresponding to given index\n",
        "\n",
        "              @return   :   features - tensot containing:\n",
        "                              * 1-hot embedding of protein sequence\n",
        "                              * Embedding from esm-1b\n",
        "                              * Embedding from ProtTrans / ProteinBERT\n",
        "              @return   :   labels - tensor containing normalised values of protein:\n",
        "                              * SS3 Class Indicies\n",
        "              @return   :   protein_len - length of protein sequence\n",
        "              @return   :   protein - protein name\n",
        "              @return   :   seq - protein sequence string\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"!  @brief  :   Fetch protein, sequence & sequence Length at given index \"\"\"\n",
        "        protein = self.protein_list[idx]\n",
        "        sequence = self.sequence_list[idx]\n",
        "        protein_len = self.protein_len_list[idx]\n",
        "\n",
        "\n",
        "        \"\"\"!  @brief  :   Load & Process Label Data for the Protein \"\"\"\n",
        "        try:\n",
        "          labels = np.load(os.path.join(LABEL_DIR, protein + \".npy\"), allow_pickle=True)\n",
        "          # normalize specific labels\n",
        "\n",
        "          ss3_indices = np.array([SS3_CLASSES.index(aa) if aa in SS3_CLASSES else -1 for aa in labels[:, 4]])\n",
        "        except:\n",
        "          print(\"no label\")\n",
        "\n",
        "        \"\"\"!  @brief  :   Fetch Embeddings & One Hot encodings \"\"\"\n",
        "        onehot_path =  os.path.join(ONEHOT_INPUT_DIR, protein + \"_1hot.npy\")\n",
        "        one_hot_enc = one_hot(sequence)\n",
        "\n",
        "        ## Model 1\n",
        "        embedding1 = self.pre_trained_model1.generate_embedding(sequence, protein)\n",
        "\n",
        "        ## Model 2\n",
        "        embedding2 = self.pre_trained_model2.generate_embedding(sequence, protein)\n",
        "\n",
        "        \"\"\"!  @brief  :   Concatenate Features \"\"\"\n",
        "        features = np.concatenate((one_hot_enc, embedding1, embedding2), axis=1)\n",
        "        print(protein,np.shape(features))\n",
        "\n",
        "        return torch.FloatTensor(features), torch.LongTensor(ss3_indices), self.protein_len_list[idx], self.protein_list[idx], self.sequence_list[idx]\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"!  Collate function for data read from text file per batch\n",
        "\n",
        "              @return   :   padded_features - Concatenation of feature vectors padded with 0s\n",
        "              @return   :   padded_labels - Concatenation of label vectors padded with 0s\n",
        "              @return   :   protein_len - length of protein sequence\n",
        "              @return   :   protein - protein name\n",
        "              @return   :   seq - protein sequence string\n",
        "        \"\"\"\n",
        "\n",
        "        # sort data by protein length in descending order\n",
        "        # sort data by protein length in descending order\n",
        "        batch.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        batch_features, batch_labels, protein_lengths = [], [], []\n",
        "        protein_names, sequences = [], []\n",
        "\n",
        "        # unpacks the sorted data into features, protein_len, and sequence\n",
        "        # features, labels, protein_len, protein, seq = zip(*data)\n",
        "        for features, labels, protein_len, protein, seq in batch:\n",
        "            batch_features.append(features)\n",
        "            batch_labels.append(labels)\n",
        "            protein_lengths.append(protein_len)\n",
        "            protein_names.append(protein)\n",
        "            sequences.append(seq)\n",
        "        # end for\n",
        "\n",
        "        # Pad feature and label tensors to ensure they have the same shape\n",
        "        # enforce_sorted=True\n",
        "        # Pad label tensors with -1 (or another invalid class index)\n",
        "        padded_features = nn.utils.rnn.pad_sequence(batch_features, batch_first=True, padding_value=-1)\n",
        "        padded_labels = nn.utils.rnn.pad_sequence(batch_labels, batch_first=True, padding_value=-1)\n",
        "\n",
        "        # returns the padded features, protein lengths,\n",
        "        # protein names, and sequences\n",
        "        return padded_features, padded_labels, torch.tensor(protein_lengths), protein_names, sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktu29vQ6JXM_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S3CeMOkqPEE"
      },
      "source": [
        "### **Regression Inference Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suVLIchEcz3a"
      },
      "outputs": [],
      "source": [
        "class Protein_Dataset_Reg(Dataset):\n",
        "    \"\"\"!  Protein Dataset Regression Class\n",
        "\n",
        "          Dataset of Protein Sequences & learned Regression Features\n",
        "\n",
        "          Loads information from File List\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name_list):\n",
        "        \"\"\"!  Protein Dataset Regression class initializer\n",
        "\n",
        "              @param[in]  :   file_name_list - List of paths to protein sequence fasta files\n",
        "        \"\"\"\n",
        "        self.protein_list = read_list(file_name_list)        ## > list of file paths to fasta per protein\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"!  Determine number of Data points in dataset\n",
        "\n",
        "              @return   :   length = length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.protein_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"!  Return protein in dataset corresponding to given index\n",
        "\n",
        "              @return   :   features - tensot containing:\n",
        "                              * 1-hot embedding of protein sequence\n",
        "                              * Embedding from esm-1b\n",
        "                              * Embedding from ProtTrans / ProteinBERT\n",
        "              @return   :   labels - tensor containing normalised values of protein:\n",
        "                              * ASA\n",
        "                              * HSE U\n",
        "                              * HSE D\n",
        "                              * phi\n",
        "                              * psi\n",
        "                              * theta\n",
        "                              * tau\n",
        "              @return   :   protein_len - length of protein sequence\n",
        "              @return   :   protein - protein name\n",
        "              @return   :   seq - protein sequence string\n",
        "        \"\"\"\n",
        "        ## file path for the protein at index idx\n",
        "        prot_path = self.protein_list[idx]\n",
        "        ## extracts the protein name from the protein path\n",
        "        protein = prot_path.split('/')[-1].split('.')[0]\n",
        "\n",
        "        ## reads the protein sequence from prot_path\n",
        "        seq = read_fasta_file(prot_path)\n",
        "        ## applies one-hot encdoing to the sequence\n",
        "        one_hot_enc = one_hot(seq)\n",
        "        ## loads EEM and ProtTrans embeddings from the numpy files\n",
        "        embedding1 = np.load(os.path.join(\"inputs/\", protein + \"_esm.npy\"))\n",
        "        embedding2 = np.load(os.path.join(\"inputs/\", protein + \"_pt.npy\"))\n",
        "        ## embedding1 = np.load(os.path.join(\"inputs/\", protein + \"_pb.npy\"))\n",
        "\n",
        "        ## load label data for the protein\n",
        "        labels = np.load(os.path.join(\"spot_1d_lm/labels\", protein + \".npy\"))\n",
        "\n",
        "        ## normalize specific labels\n",
        "        norm_labels = np.empty((labels.shape[0], 11))\n",
        "        ## normalize specific properties\n",
        "        norm_labels[:,0] = normalize_asa(labels[:,5], labels[:,3]) # normalize ASA\n",
        "        norm_labels[:,1] = normalize_hseu(labels[:,7]) # normalize HSE U\n",
        "        norm_labels[:,2] = normalize_hseu(labels[:,8]) # normalize HSE D\n",
        "\n",
        "        ## normalize dihedral angles\n",
        "        phi     = normalize_circular_angles(labels[:,9])\n",
        "        psi     = normalize_circular_angles(labels[:,10])\n",
        "        theta   = normalize_circular_angles(labels[:,11])\n",
        "        tau     = normalize_circular_angles(labels[:,12])\n",
        "\n",
        "        ## add dihedral angles into nomalized labels\n",
        "        norm_labels[:, 3:5] = phi\n",
        "        norm_labels[:, 5:7] = psi\n",
        "        norm_labels[:, 7:9] = theta\n",
        "        norm_labels[:, 9:]  = tau\n",
        "\n",
        "        ## features = np.concatenate((one_hot_enc, embedding1, embedding2), axis=1)\n",
        "        ## concatenates the one-hot encoded sequence with the two embeddings\n",
        "        features = np.concatenate((one_hot_enc, embedding1, embedding2), axis=1)\n",
        "        protein_len = len(seq)\n",
        "\n",
        "        return torch.FloatTensor(features), torch.FloatTensor(norm_labels), protein_len, protein, seq\n",
        "\n",
        "\n",
        "def text_collate_fn(data):\n",
        "    \"\"\"!  Collate function for data read from text file per batch\n",
        "\n",
        "          @return   :   padded_features - Concatenation of feature vectors padded with 0s\n",
        "          @return   :   padded_labels - Concatenation of label vectors padded with 0s\n",
        "          @return   :   protein_len - length of protein sequence\n",
        "          @return   :   protein - protein name\n",
        "          @return   :   seq - protein sequence string\n",
        "    \"\"\"\n",
        "\n",
        "    ## sort data by protein length in descending order\n",
        "    data.sort(key=lambda x: x[1], reverse=True)\n",
        "    ## unpacks the sorted data into features, protein_len, and sequence\n",
        "    features, labels, protein_len, protein, seq = zip(*data)\n",
        "\n",
        "    ## Pad feature and label tensors to ensure they have the same shape\n",
        "    padded_features = nn.utils.rnn.pad_sequence(features, batch_first=True, padding_value=0)\n",
        "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "\n",
        "    ## returns the padded features, protein lengths,\n",
        "    ## protein names, and sequences\n",
        "    return padded_features, padded_labels, protein_len, protein, seq\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oviaZb7vKJN5"
      },
      "source": [
        "### **Test Inference Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deXLMPOlKL70"
      },
      "outputs": [],
      "source": [
        "class Proteins_Dataset_Test(Dataset):\n",
        "    \"\"\"!  Protein Dataset Regression class initializer\n",
        "\n",
        "          @param[in]  :   file_name_list - List of paths to protein sequence fasta files\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name_list):\n",
        "        \"\"\"!  Protein Dataset Test class initializer\n",
        "\n",
        "              @param[in]  :   file_name_list - List of paths to protein sequence fasta files\n",
        "        \"\"\"\n",
        "        self.protein_list = read_list(file_name_list)       ## > list of file paths to fasta per protein\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"!  Determine number of Data points in dataset\n",
        "\n",
        "              @return   :   length = length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.protein_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"!  Return protein in dataset corresponding to given index\n",
        "\n",
        "              @return   :   features - tensot containing:\n",
        "                              * 1-hot embedding of protein sequence\n",
        "                              * Embedding from esm-1b\n",
        "                              * Embedding from ProtTrans / ProteinBERT\n",
        "              @return   :   protein_len - length of protein sequence\n",
        "              @return   :   protein - protein name\n",
        "              @return   :   seq - protein sequence string\n",
        "        \"\"\"\n",
        "        \"\"\"!  Return protein in dataset corresponding to given index\n",
        "\n",
        "              @return   :   features - tensot containing:\n",
        "                              * 1-hot embedding of protein sequence\n",
        "                              * Embedding from esm-1b\n",
        "                              * Embedding from ProtTrans / ProteinBERT\n",
        "              @return   :   labels - tensor containing normalised values of protein:\n",
        "                              * SS3 Class Indicies\n",
        "              @return   :   protein_len - length of protein sequence\n",
        "              @return   :   protein - protein name\n",
        "              @return   :   seq - protein sequence string\n",
        "        \"\"\"\n",
        "        ## file path for the protein at index idx\n",
        "        prot_path = self.protein_list[idx]\n",
        "        ## extracts the protein name from the protein path\n",
        "        protein = prot_path.split('/')[-1].split('.')[0]\n",
        "        ##\n",
        "\n",
        "        \"\"\"!  @brief  :  Determine Protein Sequence & length \"\"\"\n",
        "        seq = read_fasta_file(prot_path)\n",
        "        protein_len = len(seq)\n",
        "\n",
        "\n",
        "        \"\"\"!  @brief  :   Load One Hot Encoding \"\"\"\n",
        "        ## Check if one-hot encoding pregenerated\n",
        "        onehot_path =  os.path.join(ONEHOT_INPUT_DIR, protein + \"_1hot.npy\")\n",
        "        if os.path.isfile(onehot_path):\n",
        "          one_hot_enc = np.load(onehot_path)\n",
        "        else:\n",
        "          ## Else Generate and save\n",
        "          seq = read_fasta_file(prot_path)\n",
        "          one_hot_enc = one_hot(seq)\n",
        "          np.save(onehot_path, one_hot_enc)\n",
        "\n",
        "\n",
        "        \"\"\"!  @brief  :   Load Embeddings from pre-trained models \"\"\"\n",
        "        ## Model 1\n",
        "        embedding1 = self.pre_trained_model1.load_embeddings(protein)\n",
        "\n",
        "        ## Model 2\n",
        "        embedding2 = self.pre_trained_model2.load_embeddings(protein)\n",
        "\n",
        "        #print(\"embedding1\", np.shape(embedding1))\n",
        "        #print(\"embedding2\", np.shape(embedding2))\n",
        "        #print(\"one hot\",np.shape(one_hot_enc))\n",
        "\n",
        "        \"\"\"!  @brief  :   Verify all Files are valid and exist \"\"\"\n",
        "        assert(embedding1.any() != None)\n",
        "        assert(embedding2.any() != None)\n",
        "        assert(one_hot_enc.any() != None)\n",
        "\n",
        "\n",
        "        \"\"\"!  @brief  :   Concatenate Feature Data \"\"\"\n",
        "        features = np.concatenate((one_hot_enc, embedding1, embedding2), axis=1)\n",
        "\n",
        "\n",
        "        # returns a tuple of features, length of protein sequences,\n",
        "        # protein name, and protein sequence\n",
        "        return torch.FloatTensor(features), protein_len, protein, seq\n",
        "\n",
        "\n",
        "\n",
        "    def text_collate_fn(batch):\n",
        "        \"\"\"!  Collate function for data read from text file per batch\n",
        "\n",
        "              @return   :   padded_features - Concatenation of feature vectors padded with 0s\n",
        "              @return   :   padded_labels - Concatenation of label vectors padded with 0s\n",
        "              @return   :   protein_len - length of protein sequence\n",
        "              @return   :   protein - protein name\n",
        "              @return   :   seq - protein sequence string\n",
        "        \"\"\"\n",
        "\n",
        "        # sort data by protein length in descending order\n",
        "        batch.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        batch_features, protein_lengths = [], []\n",
        "        protein_names, sequences = [], []\n",
        "\n",
        "        # unpacks the sorted data into features, protein_len, and sequence\n",
        "        # features, labels, protein_len, protein, seq = zip(*data)\n",
        "        for features, protein_len, protein, seq in batch:\n",
        "            batch_features.append(features)\n",
        "            protein_lengths.append(protein_len)\n",
        "            protein_names.append(protein)\n",
        "            sequences.append(seq)\n",
        "        # end for\n",
        "\n",
        "        # Pad feature and label tensors to ensure they have the same shape\n",
        "        padded_features = nn.utils.rnn.pad_sequence(batch_features, batch_first=True, padding_value=0)\n",
        "\n",
        "        # returns the padded features, protein lengths,\n",
        "        # protein names, and sequences\n",
        "        return padded_features, torch.tensor(protein_lengths), protein_names, sequences\n",
        "    # end def\n",
        "# end class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfpTat56IMGA"
      },
      "source": [
        "##Dataset Initialisation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaYuBZgvC_4R"
      },
      "outputs": [],
      "source": [
        "def remove_missing_proteins(FILE_NAME):\n",
        "  \"\"\"!  Remove proteins with missing fasta or labels from dataset list\n",
        "\n",
        "        @note   :   All Proteins Missing Fasta are missing labels\n",
        "  \"\"\"\n",
        "  new_list = []\n",
        "  p_list = read_list(FILE_NAME)\n",
        "  for prot_path in tqdm(p_list):\n",
        "    protein = prot_path.split('/')[-1].split('.')[0]\n",
        "    ## Check for Fasta\n",
        "    fname = os.path.join(FASTA_DIR, protein+\".fasta\")\n",
        "    ## Check for DSSP\n",
        "    dsname = os.path.join(LABEL_DIR, protein + \".npy\")\n",
        "    if not (os.path.isfile(dsname) or os.path.isfile(fname)):\n",
        "      new_list.append(prot_path)\n",
        "\n",
        "  ## Write to New List\n",
        "  #with open(FILE_NAME,'w') as f:\n",
        "\t  #f.write('\\n'.join(new_list))\n",
        "\n",
        "  return new_list\n",
        "\n",
        "  #new_train = remove_missing_proteins(TRAIN_FILE)\n",
        "  #print(new_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmx7pwSbEtlZ"
      },
      "outputs": [],
      "source": [
        "new_val= remove_missing_proteins(TRAIN_FILE)\n",
        "print(new_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  QM,"
      ],
      "metadata": {
        "id": "UvkOqn2OqCnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAOvX1v3IR1L"
      },
      "outputs": [],
      "source": [
        "train_data = Proteins_Dataset_Class(TRAIN_FILE, TRAIN_DF, esm_test, pb_test)\n",
        "val_data = Proteins_Dataset_Class(VAL_FILE, VAL_DF,esm_test, pb_test)\n",
        "#test_data = Proteins_Dataset_Test(test_file_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4N7weMMU09Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RRyNwVDLbnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da582d4-3830-4075-e2e5-1ee37898a521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1I4Y_1_A (114, 3144)\n"
          ]
        }
      ],
      "source": [
        "features, ss3_indices, protein_len, protein, sequence = val_data.__getitem__(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKyBqCDYAt97"
      },
      "outputs": [],
      "source": [
        "print(np.shape(features))\n",
        "enc = features[:, :20]\n",
        "emb1 = features[:, 20:20+ESM_EMBEDDING_DIM]\n",
        "emb2 = features[:, 20+ESM_EMBEDDING_DIM:]\n",
        "print(sequence)\n",
        "print(enc)\n",
        "print(ss3_indices)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjiG-yXrmaty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b685a7d-bc0a-4a4f-dbb2-47fcfefa1d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size           :  10\n",
            "Train batches        :  3892\n",
            "Val batches          :  10\n"
          ]
        }
      ],
      "source": [
        "train_loader = build_train_dataset(config, train_data)\n",
        "val_loader = build_val_dataset(config, val_data)\n",
        "#test_loader = build_test_dataset(config, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1P8iJgzmcTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075bb5df-053c-40cb-ef7e-6487936a5868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking the shapes of the data...\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "4Q3M_1_A (274, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "4V17_1_A (159, 2862)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1Z5X_d1z5xe- (237, 2862)\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "3S8I_1_A (148, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1C4O_d1c4oa1 (408, 2862)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1MVF_d1mvfe- (44, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "3GZA_1_A (443, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1IN0_d1in0b1 (88, 2862)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "2CP9_1_A (64, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1MU2_d1mu2a2 (427, 2862)\n",
            "tensor([[-1, -1, -1,  ...,  0,  0,  0],\n",
            "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
            "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
            "        ...,\n",
            "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
            "        [ 0,  0,  0,  ..., -1, -1, -1],\n",
            "        [ 0,  0,  0,  ..., -1, -1, -1]])\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in train_loader:\n",
        "    x, y, plen_test, protein_name_test, sequence_test = batch\n",
        "    #print(np.shape(x), np.shape(y), np.shape(len))\n",
        "    print(y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99WahTXSLSBR"
      },
      "outputs": [],
      "source": [
        "label = np.load('/content/drive/MyDrive/11785 - Deep Learning/IDL_Project/Project/SPOT-1D-LM/spot_1d_lm/labels/4FQX_2_D.npy', allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0xJeAAXwWYY"
      },
      "outputs": [],
      "source": [
        "print(np.shape(x), np.shape(y), len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBmg_kBKPnHY"
      },
      "outputs": [],
      "source": [
        "print(label[:,4], label[:,5])\n",
        "print(ss3_indices)\n",
        "ss3 = ss3_indices\n",
        "vidx = np.where(ss3 != -1)[0] # valid indices\n",
        "ss3 = ss3[vidx]\n",
        "sequence = ''.join(label[vidx, 3])\n",
        "print(seq)\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY3ypKsOI4Yn"
      },
      "source": [
        "##Test/Check Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIkuSzJ2Ge_s"
      },
      "source": [
        "# Dataloaders\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bqpqATtGz-6"
      },
      "source": [
        "## Build Dataloader Helpers\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFFWQXluG6Le"
      },
      "source": [
        "### **Train Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzFbyNWCGjik"
      },
      "outputs": [],
      "source": [
        "def build_train_dataset(config, train_data):\n",
        "  \"\"\"!  Build Dataloader for Training Dataset\n",
        "        @param[in]  :   config = parameter configuration dictionary\n",
        "        @param[in]  :   train_data = training Dataset\n",
        "\n",
        "        @return     :   batch of protein sequences, features, labels, protein lengths, and protein names\n",
        "  \"\"\"\n",
        "  ## Define dataloader object\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    collate_fn  = train_data.collate_fn,\n",
        "    num_workers = 0,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        "  )\n",
        "\n",
        "  ## Print Summary of Training Data\n",
        "  print(\"Batch size           : \", config['batch_size'])\n",
        "  print(\"Train batches        : \", train_loader.__len__())\n",
        "\n",
        "  return train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTVuVrMoG_92"
      },
      "source": [
        "### **Validation Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMd-y42UG_A0"
      },
      "outputs": [],
      "source": [
        "def build_val_dataset(config, val_data):\n",
        "  \"\"\"!  Build Dataloader for Validation Dataset\n",
        "        @param[in]  :   config = parameter configuration dictionary\n",
        "        @param[in]  :   val_data = validation Dataset\n",
        "\n",
        "        @return     :   batch of protein sequences, features, labels, protein lengths, and protein names\n",
        "  \"\"\"\n",
        "  ## Define dataloader object\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    collate_fn  = val_data.collate_fn,\n",
        "    num_workers = 0,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        "  )\n",
        "\n",
        "  ## Print Summary of Validation Data\n",
        "  print(\"Val batches          : \", val_loader.__len__())\n",
        "\n",
        "  return val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjfNUIcbHGvs"
      },
      "source": [
        "### **Test Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr5vd8e1HJPD"
      },
      "outputs": [],
      "source": [
        "def build_test_dataset(config, test_data):\n",
        "  \"\"\"!  Build Dataloader for Test Dataset\n",
        "        @param[in]  :   config = parameter configuration dictionary\n",
        "        @param[in]  :   test_data = Test Dataset\n",
        "\n",
        "        @return     :   batch of protein sequences, features, protein lengths, and protein names\n",
        "  \"\"\"\n",
        "  ## Define dataloader object\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    collate_fn = test_data.collate_fn,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        "  )\n",
        "  ## Print Summary of Test Data\n",
        "  print(\"Test batches         : \", test_loader.__len__())\n",
        "\n",
        "  return test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AeaW0j0IxLs"
      },
      "source": [
        "##Test/Check Dataloaders\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "yL5iLB41Kv_Z",
        "outputId": "bf0814cb-57ab-4115-9e39-c7adcec2ae4c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e54a889adbf7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_train_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#val_loader = build_val_dataset(config, val_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#test_loader = build_test_dataset(config, test_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "train_loader = build_train_dataset(config, train_data)\n",
        "#val_loader = build_val_dataset(config, val_data)\n",
        "#test_loader = build_test_dataset(config, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NOPhfUQ0KLp"
      },
      "source": [
        "# Model Architecture Definitions\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTS-3iE9x7Vy"
      },
      "source": [
        "## Utilities and Helper Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlgAACIlx_wX"
      },
      "source": [
        "### **Permute Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US8HEAYByB5C"
      },
      "outputs": [],
      "source": [
        "class PermuteBlock(torch.nn.Module):\n",
        "    \"\"\"!  Permute Block Class\n",
        "\n",
        "          Defines function to reshape network layers\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFFtn34n03e_"
      },
      "source": [
        "##ResNet & Conv Blocks\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl67n3B21IPa"
      },
      "source": [
        "### **Custom Conv1d Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7nWenL41rqm"
      },
      "outputs": [],
      "source": [
        "class conv3x3(torch.nn.Module):\n",
        "    \"\"\"!  3x3 Conv1d Layer Class\n",
        "\n",
        "          3x3 Convolution with Padding\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels, stride=1):\n",
        "        \"\"\"!  3x3 Conv1d Layer Class Initializer\n",
        "\n",
        "              @param[in]  :   input_channels - number of input channels\n",
        "              @param[in]  :   output_channels - number of output channels\n",
        "              @param[in]  :   stride - stride of the convolution\n",
        "        \"\"\"\n",
        "        super(conv3x3, self).__init__()\n",
        "        self.conv = torch.nn.Conv1d(input_channels, output_channels, kernel_size=7, stride=stride,\n",
        "                     padding=3, bias=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"!  3x3 Conv1d Layer Forward Pass\n",
        "\n",
        "              @param[in]    :   x - input tensor\n",
        "              @return       :   x - output tensor\n",
        "        \"\"\"\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UshxGK6O3nKA"
      },
      "outputs": [],
      "source": [
        "class conv5x5(torch.nn.Module):\n",
        "    \"\"\"!  5x5 Conv1d Layer Class\n",
        "\n",
        "          5x5 Convolution with Padding\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels, stride=1):\n",
        "        \"\"\"!  5x5 Conv1d Layer Class Initializer\n",
        "\n",
        "              @param[in]  :   input_channels - number of input channels\n",
        "              @param[in]  :   output_channels - number of output channels\n",
        "              @param[in]  :   stride - stride of the convolution\n",
        "        \"\"\"\n",
        "        super(conv5x5, self).__init__()\n",
        "        self.conv = torch.nn.Conv1d(input_channels, output_channels, kernel_size=9, stride=stride,\n",
        "                     padding=4, bias=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"!  5x5 Conv1d Layer Forward Pass\n",
        "\n",
        "              @param[in]    :   x - input tensor\n",
        "              @return       :   x - output tensor\n",
        "        \"\"\"\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7FJhbow3xsx"
      },
      "outputs": [],
      "source": [
        "class conv7x7(torch.nn.Module):\n",
        "    \"\"\"!  7x7 Conv1d Layer Class\n",
        "\n",
        "          7x7 Convolution with Padding\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels, stride=1):\n",
        "        \"\"\"!  7x7 Conv1d Layer Class Initializer\n",
        "\n",
        "              @param[in]  :   input_channels - number of input channels\n",
        "              @param[in]  :   output_channels - number of output channels\n",
        "              @param[in]  :   stride - stride of the convolution\n",
        "        \"\"\"\n",
        "        super(conv7x7, self).__init__()\n",
        "        self.conv = nn.Conv1d(input_channels, output_channels, kernel_size=15, stride=stride,\n",
        "                     padding=7, bias=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"!  7x7 Conv1d Layer Forward Pass\n",
        "\n",
        "              @param[in]    :   x - input tensor\n",
        "              @return       :   x - output tensor\n",
        "        \"\"\"\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmOzT_E38YE"
      },
      "source": [
        "### **Custom Residual Blocks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCmUe4zn4FcS"
      },
      "outputs": [],
      "source": [
        "class BasicBlock3x3(nn.Module):\n",
        "    \"\"\"!  Basic 3x3 Residual Block Class\n",
        "\n",
        "          Input -> Conv3x3 -> BN -> ReLU -> Conv3x3 -> BN -> Downsample -> Output\n",
        "               \\___________________________________________________________/ (Residual Connection)\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, stride=1, downsample=None):\n",
        "        \"\"\"!  Basic 3x3 Residual Block Initializer\n",
        "\n",
        "              @param[in]  :   input_channels - number of input channels\n",
        "              @param[in]  :   output_channels - number of output channels\n",
        "              @param[in]  :   stride - stride of the convolution\n",
        "              @param[in]  :   downsample - boolean var determining if downsampling is required\n",
        "        \"\"\"\n",
        "        super(BasicBlock3x3, self).__init__()\n",
        "        self.conv1 = conv3x3(input_channels, output_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop1 = nn.Dropout(p=0.5)\n",
        "        self.conv2 = conv3x3(output_channels, output_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"!  Basic 3x3 Residual Block Forward Pass\n",
        "\n",
        "              @param[in]    :   x - input tensor\n",
        "              @return       :   out - output tensor\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.drop1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlTtPLdh524R"
      },
      "outputs": [],
      "source": [
        "class BasicBlock5x5(nn.Module):\n",
        "    \"\"\"!  Basic 5x5 Residual Block Class\n",
        "\n",
        "          Input -> Conv5x5 -> BN -> ReLU -> Conv3x3 -> BN -> Downsample -> Output\n",
        "               \\___________________________________________________________/ (Residual Connection)\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, stride=1, downsample=None):\n",
        "        \"\"\"!  Basic 5x5 Residual Block Initializer\n",
        "\n",
        "              @param[in]  :   input_channels - number of input channels\n",
        "              @param[in]  :   output_channels - number of output channels\n",
        "              @param[in]  :   stride - stride of the convolution\n",
        "              @param[in]  :   downsample - boolean var determining if downsampling is required\n",
        "        \"\"\"\n",
        "        super(BasicBlock5x5, self).__init__()\n",
        "        self.conv1 = conv5x5(input_channels, output_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop1 = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.conv2 = conv5x5(output_channels, output_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"!  Basic 5x5 Residual Block Forward Pass\n",
        "\n",
        "              @param[in]    :   x - input tensor\n",
        "              @return       :   out1 - output tensor\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.drop1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        d = residual.shape[2] - out.shape[2]\n",
        "\n",
        "        out1 = residual + out\n",
        "        out1 = self.relu(out1)\n",
        "\n",
        "        return out1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeOwz_216YkP"
      },
      "outputs": [],
      "source": [
        "class BasicBlock7x7(nn.Module):\n",
        "    \"\"\"!  Basic 7x7 Residual Block Class\n",
        "\n",
        "          Input -> Conv7x7 -> BN -> ReLU -> Conv3x3 -> BN -> Downsample -> Output\n",
        "               \\___________________________________________________________/ (Residual Connection)\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, stride=1, downsample=None):\n",
        "        \"\"\"!  Basic 7x7 Residual Block Initializer\n",
        "\n",
        "              @param[in]  :   input_channels - number of input channels\n",
        "              @param[in]  :   output_channels - number of output channels\n",
        "              @param[in]  :   stride - stride of the convolution\n",
        "              @param[in]  :   downsample - boolean var determining if downsampling is required\n",
        "        \"\"\"\n",
        "        super(BasicBlock7x7, self).__init__()\n",
        "        self.conv1 = conv7x7(input_channels, output_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop1 = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.conv2 = conv7x7(output_channels, output_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"!  Basic 7x7 Residual Block Forward Pass\n",
        "\n",
        "              @param[in]    :   x - input tensor\n",
        "              @return       :   out1 - output tensor\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.drop1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        d = residual.shape[2] - out.shape[2]\n",
        "\n",
        "        out1 = residual + out\n",
        "        out1 = self.relu(out1)\n",
        "\n",
        "        return out1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCT5pIVb9rvG"
      },
      "source": [
        "## LSTMs & RNNs\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKps3sk1A2FE"
      },
      "source": [
        "## SPOT-1D-Single Architectures\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JK0uSfTA_oD"
      },
      "source": [
        "### **Model 1&4  Architecture - 2 Layer BiLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEARtgV699fk"
      },
      "outputs": [],
      "source": [
        "class TwoLayerBLSTM(nn.Module):\n",
        "    \"\"\"!  Two Layer BiLSTM Class\n",
        "\n",
        "          SPOT-1D-Single Model 1 Architecture (for Classification)\n",
        "          SPOT-1D-Single Model 4 Architecture (for Regression)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=2862, hidden_dim=1024, output_size=3, dropout=0.5, classification=True,\n",
        "                 ih_weight_init_fn=None, hh_weight_init_fn=None, linear_weight_init_fn=None,\n",
        "                 linear_activation_fn=torch.nn.ReLU()):\n",
        "        \"\"\"!  Two Layer BiLSTM Class Initializer\n",
        "\n",
        "              @param[in]  :   input_size - input dimension\n",
        "              @param[in]  :   hidden_dim - size of lstm hidden layer\n",
        "              @param[in]  :   output_size - output dimension / number of classes\n",
        "              @param[in]  :   dropout - dropout probability\n",
        "              @param[in]  :   classification - boolean var determining if classification or Regression\n",
        "              @param[in]  :   ih_weight_init_fn - function for initialising input-hidden weights\n",
        "              @param[in]  :   hh_weight_init_fn - function for initialising hidden-hidden weights\n",
        "              @param[in]  :   linear_weight_init_fn - function for initialising linear layer weights\n",
        "              @param[in]  :   linear_activation_fn - activation function for linear layer\n",
        "        \"\"\"\n",
        "        super(TwoLayerBLSTM, self).__init__()\n",
        "        self.linear_weight_init_fn = linear_weight_init_fn\n",
        "        self.ih_weight_init_fn = ih_weight_init_fn\n",
        "        self.hh_weight_init_fn = hh_weight_init_fn\n",
        "        self.linear_activation_fn = linear_activation_fn\n",
        "\n",
        "        self.classification = classification\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=self.hidden_dim, num_layers=2, batch_first=True,\n",
        "                             bidirectional=True, dropout=dropout)\n",
        "        self.drop1 = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features=2*self.hidden_dim, out_features=1000)\n",
        "        self.linear2 = nn.Linear(in_features=1000, out_features=1000)\n",
        "        self.linear3 = nn.Linear(in_features=1000, out_features=output_size)\n",
        "\n",
        "        ## If Regression Model Add Sigmoid Layer\n",
        "        if not classification:\n",
        "          self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        ## Initialise Weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"!  Initialise Weights for 2 Layer BiLSTM\n",
        "        \"\"\"\n",
        "        ## Initialise LSTM\n",
        "        if self.ih_weight_init_fn != None and self.hh_weight_init_fn != None:\n",
        "          for name, param in self.lstm1.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                self.ih_weight_init_fn(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                self.hh_weight_init_fn(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "\n",
        "        ## Initialise Linear Layers\n",
        "        if self.linear_weight_init_fn != None:\n",
        "          self.linear_weight_init_fn(self.linear1)\n",
        "\n",
        "\n",
        "    def forward(self, x, seq_lens):\n",
        "        \"\"\"!  Two Layer BiLSTM Forward Pass\n",
        "\n",
        "              @param[in]    :   x - padded input tensor\n",
        "              @return       :   x - padded output tensor\n",
        "        \"\"\"\n",
        "        #### creating a mask of shape [B, L]\n",
        "\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, seq_lens, batch_first=True)\n",
        "        x, (hidden, cell) = self.lstm1(x)\n",
        "        x, y = nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.linear_activation_fn(self.linear1(x))\n",
        "        x = self.linear_activation_fn(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        ## If Regression Model Pass Through Sigmoid Layer\n",
        "        if not self.classification:\n",
        "          x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PhBltDfBF8Q"
      },
      "source": [
        "### **Model 2&5 Architecture - MSResNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyMnB0riB_Pb"
      },
      "outputs": [],
      "source": [
        "class MsResNet(nn.Module):\n",
        "    \"\"\"! MS ResNet Class\n",
        "\n",
        "         SPOT-1D-Single Model 2 Architecture (for Classification)\n",
        "         SPOT-1D-Single Model 5 Architecture (for Regression)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channel=2862, layers=[5, 5, 5, 1], num_classes=3, classification=True,\n",
        "                 weight_init_fn=None):\n",
        "        \"\"\"!  MSResNet Class Initializer\n",
        "\n",
        "              @param[in]  :   input_channel - input dimension\n",
        "              @param[in]  :   layers - list of required conv blocks\n",
        "              @param[in]  :   num_classes - output dimension / number of classes\n",
        "              @param[in]  :   dropout - dropout probability\n",
        "              @param[in]  :   classification - boolean var determining if classification or Regression\n",
        "              @param[in]  :   weight initialization function for Conv Layers & Linear Layer\n",
        "        \"\"\"\n",
        "        self.inplanes3 = 64\n",
        "        self.inplanes5 = 64\n",
        "        self.inplanes7 = 64\n",
        "\n",
        "        self.classification = classification\n",
        "\n",
        "        self.weight_init_fn = weight_init_fn\n",
        "\n",
        "        super(MsResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channel, 64, kernel_size=7, stride=1, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer3x3_1 = self._make_layer3(BasicBlock3x3, 64, layers[0], stride=1)\n",
        "        self.layer3x3_2 = self._make_layer3(BasicBlock3x3, 128, layers[1], stride=1)\n",
        "        self.layer3x3_3 = self._make_layer3(BasicBlock3x3, 256, layers[2], stride=1)\n",
        "        self.maxpool3 = nn.AvgPool1d(kernel_size=16, stride=1, padding=0)\n",
        "\n",
        "        self.layer5x5_1 = self._make_layer5(BasicBlock5x5, 64, layers[0], stride=1)\n",
        "        self.layer5x5_2 = self._make_layer5(BasicBlock5x5, 128, layers[1], stride=1)\n",
        "        self.layer5x5_3 = self._make_layer5(BasicBlock5x5, 256, layers[2], stride=1)\n",
        "        self.maxpool5 = nn.AvgPool1d(kernel_size=11, stride=1, padding=0)\n",
        "\n",
        "        self.layer7x7_1 = self._make_layer7(BasicBlock7x7, 64, layers[0], stride=1)\n",
        "        self.layer7x7_2 = self._make_layer7(BasicBlock7x7, 128, layers[1], stride=1)\n",
        "        self.layer7x7_3 = self._make_layer7(BasicBlock7x7, 256, layers[2], stride=1)\n",
        "        self.maxpool7 = nn.AvgPool1d(kernel_size=6, stride=1, padding=0)\n",
        "\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        ## If Regression Model Add Sigmoid Layer\n",
        "        if not classification:\n",
        "          self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        ## Initialise Weights\n",
        "        #self.init_weights()\n",
        "\n",
        "\n",
        "    def _make_layer3(self, block, planes, blocks, stride=2):\n",
        "        \"\"\"!  Construct 3x3 Conv Block Layer with given properties\n",
        "\n",
        "              @param[in]  :   block - conv block type\n",
        "              @param[in]  :   planes - output dimension\n",
        "              @param[in]  :   blocks - number of conv blocks in Layer\n",
        "              @param[in]  :   stride - block stride\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes3 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes3, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes3, planes, stride, downsample))\n",
        "        self.inplanes3 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes3, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _make_layer5(self, block, planes, blocks, stride=2):\n",
        "        \"\"\"!  Construct 5x5 Conv Block Layer with given properties\n",
        "\n",
        "              @param[in]  :   block - conv block type\n",
        "              @param[in]  :   planes - output dimension\n",
        "              @param[in]  :   blocks - number of conv blocks in Layer\n",
        "              @param[in]  :   stride - block stride\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes5 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes5, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes5, planes, stride, downsample))\n",
        "        self.inplanes5 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes5, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _make_layer7(self, block, planes, blocks, stride=2):\n",
        "        \"\"\"!  Construct 7x7 Conv Block Layer with given properties\n",
        "\n",
        "              @param[in]  :   block - conv block type\n",
        "              @param[in]  :   planes - output dimension\n",
        "              @param[in]  :   blocks - number of conv blocks in Layer\n",
        "              @param[in]  :   stride - block stride\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes7 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes7, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes7, planes, stride, downsample))\n",
        "        self.inplanes7 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes7, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x0, len):\n",
        "        \"\"\"!  MsResNet Forward Pass\n",
        "\n",
        "              @param[in]    :   x - padded in[put tensor]\n",
        "              @param[in]    :   len - length of input tensor\n",
        "              @return       :   out 1 - padded output tensor\n",
        "        \"\"\"\n",
        "        x0 = x0.permute(0, 2, 1)\n",
        "        x0 = self.conv1(x0)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "\n",
        "        x = self.layer3x3_1(x0)\n",
        "        x = self.layer3x3_2(x)\n",
        "        x = self.layer3x3_3(x)\n",
        "\n",
        "        y = self.layer5x5_1(x0)\n",
        "        y = self.layer5x5_2(y)\n",
        "        y = self.layer5x5_3(y)\n",
        "\n",
        "        z = self.layer7x7_1(x0)\n",
        "        z = self.layer7x7_2(z)\n",
        "        z = self.layer7x7_3(z)\n",
        "\n",
        "        out = x + y + z\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out1 = self.fc(out)\n",
        "\n",
        "        ## If Regression Model Pass Through Sigmoid Layer\n",
        "        if not self.classification:\n",
        "          x = self.sigmoid(x)\n",
        "\n",
        "        return out1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iomV4KFPBKCJ"
      },
      "source": [
        "### **Model 3&6 Architecture - MSResLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq6_7UylGF99"
      },
      "outputs": [],
      "source": [
        "class MsResNetLSTM(nn.Module):\n",
        "    \"\"\"! MS ResNet LSTM Class\n",
        "\n",
        "         SPOT-1D-Single Model 3 Architecture (for Classification)\n",
        "         SPOT-1D-Single Model 6 Architecture (for Regression)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channel=2862, layers=[5, 5, 5, 1], num_classes=3, dropout=0.5, classification=True,\n",
        "                 ih_weight_init_fn=None, hh_weight_init_fn=None,):\n",
        "        \"\"\"!  MSResNet Class Initializer\n",
        "\n",
        "              @param[in]  :   input_channel - input dimension\n",
        "              @param[in]  :   layers - list of required conv blocks\n",
        "              @param[in]  :   num_classes - output dimension / number of classes\n",
        "              @param[in]  :   dropout - dropout probability\n",
        "              @param[in]  :   classification - boolean var determining if classification or Regression\n",
        "              @param[in]  :   ih_weight_init_fn - function for initialising input-hidden weights\n",
        "              @param[in]  :   hh_weight_init_fn - function for initialising hidden-hidden weights\n",
        "        \"\"\"\n",
        "        self.inplanes3 = 64\n",
        "        self.inplanes5 = 64\n",
        "        self.inplanes7 = 64\n",
        "\n",
        "        self.ih_weight_init_fn = ih_weight_init_fn\n",
        "        self.hh_weight_init_fn = hh_weight_init_fn\n",
        "\n",
        "        self.classification = classification\n",
        "\n",
        "        super(MsResNetLSTM, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channel, 64, kernel_size=7, stride=1, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=4, batch_first=True,\n",
        "                             bidirectional=True, dropout=dropout)\n",
        "\n",
        "        # maxplooing kernel size: 16, 11, 6\n",
        "        self.maxpool3 = nn.AvgPool1d(kernel_size=16, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.layer5x5_1 = self._make_layer5(BasicBlock5x5, 64, layers[0], stride=1)\n",
        "        self.layer5x5_2 = self._make_layer5(BasicBlock5x5, 128, layers[1], stride=1)\n",
        "        self.layer5x5_3 = self._make_layer5(BasicBlock5x5, 256, layers[2], stride=1)\n",
        "        self.maxpool5 = nn.AvgPool1d(kernel_size=11, stride=1, padding=0)\n",
        "\n",
        "\n",
        "        self.layer7x7_1 = self._make_layer7(BasicBlock7x7, 64, layers[0], stride=1)\n",
        "        self.layer7x7_2 = self._make_layer7(BasicBlock7x7, 128, layers[1], stride=1)\n",
        "        self.layer7x7_3 = self._make_layer7(BasicBlock7x7, 256, layers[2], stride=1)\n",
        "        self.maxpool7 = nn.AvgPool1d(kernel_size=6, stride=1, padding=0)\n",
        "\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        ## If Regression Model Add Sigmoid Layer\n",
        "        if not classification:\n",
        "          self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        ## Initialise Weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"!  Initialise Weights for 2 Layer BiLSTM\n",
        "        \"\"\"\n",
        "        ## Initialise LSTM\n",
        "        if self.ih_weight_init_fn and self.hh_weight_init_fn:\n",
        "          for name, param in self.lstm1.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                self.ih_weight_init_fn(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                self.hh_weight_init_fn(param.data)\n",
        "            elif 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "\n",
        "\n",
        "    def _make_layer3(self, block, planes, blocks, stride=2):\n",
        "        \"\"\"!  Construct 3x3 Conv Block Layer with given properties\n",
        "\n",
        "              @param[in]  :   block - conv block type\n",
        "              @param[in]  :   planes - output dimension\n",
        "              @param[in]  :   blocks - number of conv blocks in Layer\n",
        "              @param[in]  :   stride - block stride\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes3 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes3, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes3, planes, stride, downsample))\n",
        "        self.inplanes3 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes3, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _make_layer5(self, block, planes, blocks, stride=2):\n",
        "        \"\"\"!  Construct 5x5 Conv Block Layer with given properties\n",
        "\n",
        "              @param[in]  :   block - conv block type\n",
        "              @param[in]  :   planes - output dimension\n",
        "              @param[in]  :   blocks - number of conv blocks in Layer\n",
        "              @param[in]  :   stride - block stride\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes5 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes5, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes5, planes, stride, downsample))\n",
        "        self.inplanes5 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes5, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def _make_layer7(self, block, planes, blocks, stride=2):\n",
        "        \"\"\"!  Construct 7x7 Conv Block Layer with given properties\n",
        "\n",
        "              @param[in]  :   block - conv block type\n",
        "              @param[in]  :   planes - output dimension\n",
        "              @param[in]  :   blocks - number of conv blocks in Layer\n",
        "              @param[in]  :   stride - block stride\n",
        "        \"\"\"\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes7 != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv1d(self.inplanes7, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes7, planes, stride, downsample))\n",
        "        self.inplanes7 = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes7, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x0, seq_lens):\n",
        "        \"\"\"!  MsResNetLSTM Forward Pass\n",
        "              @param[in]    :   x0 - padded in[put tensor]\n",
        "              @param[in]    :   seq_lens - length of input tensor\n",
        "              @return       :   out 1 - padded output tensor\n",
        "        \"\"\"\n",
        "        x0 = x0.permute(0,2,1)\n",
        "        x0 = self.conv1(x0)\n",
        "        x0 = self.bn1(x0)\n",
        "        x0 = self.relu(x0)\n",
        "\n",
        "        x = x0.permute(0,2,1)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, seq_lens, batch_first=True)\n",
        "        x, (hidden, cell) = self.lstm(x)\n",
        "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0)\n",
        "        x = x.permute(0,2,1)\n",
        "\n",
        "        y = self.layer5x5_1(x0)\n",
        "        y = self.layer5x5_2(y)\n",
        "        y = self.layer5x5_3(y)\n",
        "\n",
        "        z = self.layer7x7_1(x0)\n",
        "        z = self.layer7x7_2(z)\n",
        "        z = self.layer7x7_3(z)\n",
        "\n",
        "        out = x+y+z\n",
        "        out = out.permute(0,2,1)\n",
        "        out1 = self.fc(out)\n",
        "\n",
        "        ## If Regression Model Pass Through Sigmoid Layer\n",
        "        if not self.classification:\n",
        "          x = self.sigmoid(x)\n",
        "\n",
        "        return out1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndt-7DycHW1F"
      },
      "source": [
        "### **Ensemble Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X-4lhMtHZOE"
      },
      "outputs": [],
      "source": [
        "class EnsembleNetwork(nn.Module):\n",
        "    \"\"\"! SPOT-1D-Single Ensemble Network Class\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=2862, num_classes=3, hidden_dim=1000, sequence_length=1024,classification=True):\n",
        "        \"\"\"!  SPOT-1D-Single Ensemble Class Initializer\n",
        "\n",
        "              @param[in]  :   input_size - input dimension\n",
        "              @param[in]  :   sequence_length - sequence length (max?)\n",
        "              @param[in]  :   num_classes - output dimension / number of classes\n",
        "              @param[in]  :   hidden_dim - dimension of lstm hidden layer\n",
        "              @param[in]  :   classification - boolean var determining if classification or Regression\n",
        "        \"\"\"\n",
        "        super(EnsembleNetwork, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.classification = classification\n",
        "\n",
        "        # Initialize the three models with the correct input sizes\n",
        "        self.model1 = TwoLayerBLSTM(input_size=input_size, hidden_dim=hidden_dim)\n",
        "        self.model2 = MsResNet(input_channel=input_size, layers=[5, 5, 5, 1], num_classes=num_classes)\n",
        "        self.model3 = MsResNetLSTM(input_channel=input_size, layers=[5, 5, 5, 1], num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, seq_lens):\n",
        "        \"\"\"!  SPOT-1D-Single Ensemble Forward Pass\n",
        "\n",
        "              @param[in]    :   x - padded in[put tensor]\n",
        "              @param[in]    :   seq_lens - length of input tensor\n",
        "              @return       :   out 1 - padded output tensor\n",
        "        \"\"\"\n",
        "        # Assume x is of shape [batch_size, sequence_length, 20]\n",
        "\n",
        "        # Pass the input through each model\n",
        "        output1 = self.model1(x, seq_lens)\n",
        "        output2 = self.model2(x, seq_lens)\n",
        "        output3 = self.model3(x, seq_lens)\n",
        "\n",
        "        # Average the logits from each model\n",
        "        logits = (output1 + output2 + output3) / 3.0\n",
        "\n",
        "        # Apply softmax or another final activation if needed\n",
        "        return F.log_softmax(logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKbPEBHmfYhX"
      },
      "outputs": [],
      "source": [
        "model = gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = EnsembleNetwork().to(DEVICE)\n",
        "#summary(model, [x.to(DEVICE), plen_test.to(DEVICE)])\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/test_ab1.pth'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(x.to(DEVICE), plen_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWq0sT6v1s8l",
        "outputId": "46b2f77e-e7be-4667-8c77-3e21dada66ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.2878, -2.2909, -0.4733],\n",
              "         [-1.5664, -1.1067, -0.7753],\n",
              "         [-1.0787, -1.0664, -1.1529],\n",
              "         ...,\n",
              "         [-1.3340, -1.5215, -0.6574],\n",
              "         [-0.8472, -1.0706, -1.4758],\n",
              "         [-1.9390, -0.5682, -1.2392]],\n",
              "\n",
              "        [[-1.0164, -1.3802, -0.9504],\n",
              "         [-1.6903, -1.2941, -0.6136],\n",
              "         [-1.3803, -1.1311, -0.8538],\n",
              "         ...,\n",
              "         [-1.7576, -0.7307, -1.0614],\n",
              "         [-1.1781, -0.6812, -1.6815],\n",
              "         [-1.3797, -0.6130, -1.5768]],\n",
              "\n",
              "        [[-0.9875, -1.5670, -0.8703],\n",
              "         [-0.9853, -1.1946, -1.1274],\n",
              "         [-0.8875, -1.5658, -0.9691],\n",
              "         ...,\n",
              "         [-2.1115, -1.8622, -0.3235],\n",
              "         [-1.3138, -1.0003, -1.0122],\n",
              "         [-1.1532, -0.8016, -1.4450]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.8275, -1.3555, -1.1873],\n",
              "         [-0.7908, -1.3746, -1.2255],\n",
              "         [-0.7934, -1.4555, -1.1570],\n",
              "         ...,\n",
              "         [-1.5901, -2.3864, -0.3508],\n",
              "         [-1.1355, -1.3528, -0.8670],\n",
              "         [-0.7973, -1.4119, -1.1849]],\n",
              "\n",
              "        [[-1.1917, -1.4005, -0.7989],\n",
              "         [-1.3564, -1.2696, -0.7733],\n",
              "         [-0.8460, -1.5694, -1.0141],\n",
              "         ...,\n",
              "         [-1.7019, -0.7415, -1.0752],\n",
              "         [-1.1518, -1.3107, -0.8812],\n",
              "         [-1.8474, -1.2763, -0.5740]],\n",
              "\n",
              "        [[-1.2611, -1.7905, -0.5983],\n",
              "         [-1.8350, -1.4387, -0.5056],\n",
              "         [-1.3448, -0.9718, -1.0188],\n",
              "         ...,\n",
              "         [-1.2774, -1.2176, -0.8549],\n",
              "         [-1.4412, -0.8914, -1.0405],\n",
              "         [-1.1734, -1.5323, -0.7452]]], device='cuda:0',\n",
              "       grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCsx-PcCq5DS"
      },
      "source": [
        "# Model Training\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lG4BdZyGVJv"
      },
      "source": [
        "##Training Configuration\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJm1HHRv5KqT"
      },
      "source": [
        "### **Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21pZZaaLv7h2"
      },
      "outputs": [],
      "source": [
        "def build_optimizer(model, config):\n",
        "  \"\"\"!   Build Optimizer for Model\n",
        "          @param[in]  :   model = model to train\n",
        "          @param[in]  :   config = parameter configuration dictionary\n",
        "\n",
        "          @return     :   optimizer\n",
        "  \"\"\"\n",
        "  if config['optimizer'] == \"sgd\":\n",
        "      optimizer = torch.optim.SGD(model.parameters(),\n",
        "                              lr=config['lr'], momentum=config['momentum'],\n",
        "                              weight_decay=config['weight_decay'])\n",
        "  elif config['optimizer'] == \"adam\":\n",
        "      optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                               lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "  elif config['optimizer'] == \"nadam\":\n",
        "    optimizer = torch.optim.NAdam(model.parameters(),\n",
        "                               lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "  elif config['optimizer'] == \"rms\":\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(),\n",
        "                               lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "  elif config['optimizer'] == \"adagrad\":\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(),\n",
        "                               lr=config['lr'], lr_decay=0.01,weight_decay=config['weight_decay'])\n",
        "  elif config['lbfgs'] == \"rms\":\n",
        "    torch.optim.LBFGS(model.parameters(),\n",
        "                               lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10XEJJGN5QLI"
      },
      "source": [
        "### **Criterion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmZafoq1wNM-"
      },
      "outputs": [],
      "source": [
        "def build_criterion(model, config):\n",
        "  \"\"\"!   Build Criterion for Model\n",
        "          @param[in]  :   model = model to train\n",
        "          @param[in]  :   config = parameter configuration dictionary\n",
        "\n",
        "          @return     :   torch criterion\n",
        "  \"\"\"\n",
        "  if config['criterion'] == \"BCE\":\n",
        "    criterion = torch.nn.BCELoss()\n",
        "  elif config['criterion'] == \"cross_entropy\":\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "  return criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYGyOroWgZNt"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sVSBPqeRTAM"
      },
      "outputs": [],
      "source": [
        "def compute_masked_loss():\n",
        "  \"\"\"!   Build Optimizer for Network\n",
        "          @param[in]  :   outputs - output tensor\n",
        "          @param[in]  :   SS3 indicies - output classifier data\n",
        "\n",
        "          @return     :   loss\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkQSBcEC5SQd"
      },
      "source": [
        "### **Scheduler**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUeEoCzYWar_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF_CTYDNRAWJ"
      },
      "outputs": [],
      "source": [
        "scheduler   = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                         mode='min',\n",
        "                                                         factor=0.8,\n",
        "                                                         patience=3,\n",
        "                                                         verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HgG29sN5Ttc"
      },
      "source": [
        "### **Scaler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skIstE3RRux_"
      },
      "outputs": [],
      "source": [
        "scaler      = torch.cuda.amp.GradScaler()  # Initialize the gradient scaler for mixed-precision training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHiJSNpwQKj"
      },
      "source": [
        "## Classification Train & Validate Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUnggMELwYUc"
      },
      "source": [
        "### **Train Epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4GN7nEDRwD_"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, scaler):\n",
        "    \"\"\"!   Run a single Training Epoch\n",
        "          @param[in]  :   model = NN network to train\n",
        "          @param[in]  :   train_loader = dataloader for training dataset\n",
        "          @param[in]  :   optimizer = training optimizer\n",
        "          @param[in]  :   criterion = loss function\n",
        "          @param[in]  :   scaler = scaler for mixed precision learning\n",
        "\n",
        "          @return     :   model = trained NN network\n",
        "          @return     :   ep_loss = average loss for epoch\n",
        "          @return     :   ep_acc = average accuracy for epoch\n",
        "    \"\"\"\n",
        "    model.train() # Set Model in Training mode\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    ## Loop through points in dataset\n",
        "    for i, (batch) in enumerate(train_loader):\n",
        "          ## Initialise Gradient\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          ## Move Data to Device (Ideally GPU)\n",
        "          x, y, lengths, protein_names, sequences = batch\n",
        "          x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "          with torch.cuda.amp.autocast():\n",
        "              ## Predict output with model & calculate loss\n",
        "              ## Forward Propagation\n",
        "              outputs = model(x, lengths)\n",
        "              #print(\"x\", x.size())\n",
        "              #print(\"out, prob, y\",raw_outputs.size(), prob_dist.size(), y.size())\n",
        "              loss    = criterion(outputs, y)\n",
        "\n",
        "          ## Update no. of correct predictions & loss as we iterate\n",
        "          total_loss += float(loss.item())\n",
        "          total_accuracy += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "\n",
        "\n",
        "          ## Update Batch Bar\n",
        "          batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(total_accuracy*100 / (i + 1))),\n",
        "                              lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "          batch_bar.update()\n",
        "\n",
        "          ## Log in W&B\n",
        "          wandb.log({\"batch loss\": loss.item})\n",
        "\n",
        "          ## Backward pass with scaled gradients\n",
        "          scaler.scale(loss).backward()\n",
        "          scaler.step(optimizer)  # Update model parameters\n",
        "          scaler.update()  # Update the scale for next iteration\n",
        "\n",
        "          scheduler.step()\n",
        "\n",
        "          ### Release memory\n",
        "          del x, y, lengths, protein_names, sequences\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "    ## Calculate Average Loss for Epoch\n",
        "    batch_bar.close()\n",
        "    ep_loss = float(total_loss / len(train_loader))\n",
        "    ep_acc = float(total_accuracy / len(train_loader))\n",
        "    lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    return model, ep_loss, ep_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZkOLt7ewg0s"
      },
      "source": [
        "### **Validate Epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKfxtuwuTocn"
      },
      "outputs": [],
      "source": [
        "def eval(model, val_loader):\n",
        "    \"\"\"!  Evaluate Model with Validation Data Set\n",
        "          @param[in]  :   model = trained NN network to evaluate\n",
        "          @param[in]  :   val_loader = dataloader for validation dataset\n",
        "\n",
        "          @return     :   ep_loss = average loss for epoch\n",
        "          @return     :   ep_acc = average accuracy for epoch\n",
        "    \"\"\"\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, v_acc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    ## Loop through points in dataset\n",
        "    for i, (batch) in enumerate(val_loader):\n",
        "\n",
        "        ## Move Data to Device (Ideally GPU)\n",
        "        x, y, lengths, protein_names, sequences = batch\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            ### Loss Calculation\n",
        "            outputs = model(x, lengths)\n",
        "            loss    = criterion(outputs, y)\n",
        "\n",
        "        vloss      += float(loss.item())\n",
        "        v_acc       += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "\n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(v_acc / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del x, y, raw_outputs, prob_dist\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    v_acc    /= len(val_loader)\n",
        "\n",
        "    return vloss, v_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8S9U6E8Tszw"
      },
      "source": [
        "### **Training & Val Full**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfXKcDY_pI7B"
      },
      "outputs": [],
      "source": [
        "def train(model, model_config, finish= True):\n",
        "    \"\"\"!  Train NN\n",
        "          @param[in]  :   model = NN network to train\n",
        "          @param[in]  :   model_config = dictionary of model params\n",
        "    \"\"\"\n",
        "    ## Reset cache & torch\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "\n",
        "    '''!  @brief  :   INIT & SETUP '''\n",
        "    ## Building Training and Validation Datasets\n",
        "    train_loader = build_train_dataset(model_config, train_data)\n",
        "    val_loader = build_val_dataset(model_config, val_data)\n",
        "\n",
        "    ## Define Optimizer, Criterion, Scheduler & Scaler\n",
        "    optimizer = build_optimizer(model, model_config)\n",
        "    scheduler   = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                         mode='min',\n",
        "                                                         factor=0.8,\n",
        "                                                         patience=3,\n",
        "                                                         verbose=True)\n",
        "\n",
        "    ## Set parameter to keep track of best epoch accuracy\n",
        "    best_lev_dist = float(\"inf\")\n",
        "\n",
        "\n",
        "    ## Loop through Epochs & Train/Evaluate Model\n",
        "    for epoch in range(model_config['epochs']):\n",
        "\n",
        "      '''!  @brief  :   RUN TRAIN & EVAL FUNCTIONS '''\n",
        "      ## Determine Current Learning Rate for Epoch\n",
        "      curr_lr  = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "      ## Train Model -> Obtain Loss & Accuracy\n",
        "      train_model, train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, scaler)\n",
        "\n",
        "      ## Test Against Validation Set --> Obtain Loss & Accuracy\n",
        "      val_loss, val_dist = eval(model, val_loader)\n",
        "\n",
        "      scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "      '''!  @brief  :   LOG RESULTS '''\n",
        "      print(\"\\t Epoch {}/{}:\".format(epoch + 1,model_config['epochs']))\n",
        "      print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "      print(\"\\tLev Dist {:.04f}%\\tVal Loss {:.04f}\".format(val_dist, val_loss))\n",
        "\n",
        "      ## Log Metrics at the End of Each Epoch\n",
        "      ## Create W&B dic of Epoch metrics\n",
        "      metrics = {\n",
        "          \"train_loss\":train_loss,\n",
        "          \"train_acc\": train_acc*100,\n",
        "          'val_dist': val_dist,\n",
        "          'valid_loss': val_loss,\n",
        "          'lr': curr_lr,\n",
        "      }\n",
        "\n",
        "      ## Log Metrics to W&B\n",
        "      wandb.log(metrics)\n",
        "\n",
        "      '''!  @brief  :   ARTIFACT & MODEL CHECKPOINT '''\n",
        "      ## If Improved Accuracy, Update Accuracy & Model/Optimizer States\n",
        "      if val_dist < best_lev_dist:\n",
        "        best_lev_dist = val_dist\n",
        "\n",
        "        ## Save to Google Drive\n",
        "        torch.save({\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_dist': val_dist,\n",
        "              'epoch': epoch\n",
        "              }, (CHECKPOINT_PATH))\n",
        "\n",
        "        ## Save Checkpoint to W&B\n",
        "        #wandb.save(model_config['checkpoint_path'])\n",
        "\n",
        "\n",
        "\n",
        "      ## Create W&B Artifact\n",
        "      '''model_artifact = wandb.Artifact(run_config['model'], type='model')\n",
        "      model_artifact.add_file(\"Model\")\n",
        "\n",
        "      ## Save to W&B\n",
        "      run.log_artifact(model_artifact)'''\n",
        "\n",
        "\n",
        "    #wandb.finish()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "3euGZw37hZB6",
        "outputId": "e61b9122-87b2-4f06-c451-869eb7d6ffdd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231208_003826-sdctj9wi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/madidavis11785/project-ablations/runs/sdctj9wi' target=\"_blank\">project-submission</a></strong> to <a href='https://wandb.ai/madidavis11785/project-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/madidavis11785/project-ablations' target=\"_blank\">https://wandb.ai/madidavis11785/project-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/madidavis11785/project-ablations/runs/sdctj9wi' target=\"_blank\">https://wandb.ai/madidavis11785/project-ablations/runs/sdctj9wi</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"project-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"project-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "XsLWIxgjTyFg",
        "outputId": "b92af320-c928-4562-d639-7c343c9fde5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size           :  10\n",
            "Train batches        :  3892\n",
            "Val batches          :  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/3892 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n",
            "2DCW_1_A (42, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "3HMX_2_B (197, 2862)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "3CU2_1_A (237, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "2CFM_1_A (561, 2862)\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "2K1N_3_A (55, 2862)\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "2KSD_1_A (115, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "3P4T_1_A (403, 2862)\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "2L3S_1_A (163, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1DGN_1_A (89, 2862)\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "3AAI_1_A (94, 2862)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-226465974a70>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCURRENT_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-122-a9d6fdd991e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, model_config, finish)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m## Train Model -> Obtain Loss & Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m## Test Against Validation Set --> Obtain Loss & Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-3bd5bdd1c6dd>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, criterion, scaler)\u001b[0m\n\u001b[1;32m     33\u001b[0m               \u001b[0;31m#print(\"x\", x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m               \u001b[0;31m#print(\"out, prob, y\",raw_outputs.size(), prob_dist.size(), y.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m               \u001b[0mloss\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0;31m## Update no. of correct predictions & loss as we iterate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [10, 3], got [10, 561]"
          ]
        }
      ],
      "source": [
        "train(model, CURRENT_CONFIG)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}